<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> portfolio | Jonghyun (Thomas) Lee </title> <meta name="author" content="Jonghyun (Thomas) Lee"> <meta name="description" content="In God we trust, all others bring data. - William Edwards Deming (1900-1993)"> <meta name="keywords" content="deep generative models"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/apple-touch-icon-precomposed.png?f3b0e00b51d3560daeef2cfef2a1c566"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tomtom1103.github.io/portfolio/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?ac1a8a24b4b1b97e0b04e951186c207f"></script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/styles.min.css"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-bundle.min.css" integrity="sha256-yUoNxsvX+Vo8Trj3lZ/Y5ZBf8HlBFsB6Xwm7rH75/9E=" crossorigin="anonymous"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jonghyun</span> (Thomas) Lee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/portfolio/">portfolio <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/Tom_Resume_0228.pdf" target="_blank" rel="noopener noreferrer">cv <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">portfolio</h1> <p class="post-description">In God we trust, all others bring data. - William Edwards Deming (1900-1993)</p> </header> <article> <style>.coloured-slider{--divider-color:rgba(0,0,0,0.5);--default-handle-color:rgba(0,0,0,0.5)}</style> <hr> <h2 id="publications"><strong>Publications</strong></h2> <p><br></p> <h3 id="1-iclr-2024-compose-and-conquer-diffusion-based-3d-depth-aware-composable-image-synthesis">1. [ICLR 2024] Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis</h3> <p><br></p> <p>Conditional diffusion models usually take in text, and two different types of conditions to generate an image.</p> <ol> <li> <strong>Local conditions</strong>, which conditions the model on structural information through primitives like depth maps and canny edges.</li> <li> <strong>Global conditoins</strong>, which conditions the model on semantic information (color, identity, texture, etc).</li> </ol> <p>However, these models suffer from two limitations: they lack the ability to disentangle the relative depth of multiple local conditions, and are inable to localize global conditions to a specific area.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/cnc/1-480.webp 480w,/assets/img/portfolio/cnc/1-800.webp 800w,/assets/img/portfolio/cnc/1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/cnc/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>Limitation 1:</strong> Suppose you’d like to generate an image of two puppies with one in front of another that have the shape of the depthmaps. Models aren’t able to distinguish that one puppy has to be placed in front of the other, resulting in a fused image.</li> <li> <strong>Limitation 2:</strong> How about an elephant standing in a forest, that shares the shape (depthmap) and semantic of the forest image? Models can’t tell that the image semantics are supposed to go behind (or around) the elephant, resulting in the elephant being completely ignored.</li> </ul> <p>We sought to create a model that was able to <strong>1. Distinguish between where objects should be placed in relative depths, and 2. Localize global semantics onto a user-defined area</strong>, not just the whole image.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/cnc/2-480.webp 480w,/assets/img/portfolio/cnc/2-800.webp 800w,/assets/img/portfolio/cnc/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/cnc/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We solved this through proposing a training paradigm and a novel inference algorithm: <strong>Depth Disentanglement Training</strong> and <strong>Soft Guidance</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/cnc/ddt-480.webp 480w,/assets/img/portfolio/cnc/ddt-800.webp 800w,/assets/img/portfolio/cnc/ddt-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/cnc/ddt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>Depth Disentanglement Training</strong> parses the training data into three sub-components: the foreground, background, and mask via inpainting. The background image serves as structural information that’s initially occluded by the salient object, forcing the model to learn what’s <strong>behind</strong> another object.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/cnc/soft_guidance-480.webp 480w,/assets/img/portfolio/cnc/soft_guidance-800.webp 800w,/assets/img/portfolio/cnc/soft_guidance-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/cnc/soft_guidance.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>Soft Guidance</strong> partially masks out the pixels that are attending to certain semantics in the cross-attention layers of the model, forcing the model to attend to <strong>parts of the pixels</strong> that correspond to their semantic counterparts.</p> <img-comparison-slider class="coloured-slider"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/cnc/comparison_after-480.webp 480w,/assets/img/portfolio/cnc/comparison_after-800.webp 800w,/assets/img/portfolio/cnc/comparison_after-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/cnc/comparison_after.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/cnc/comparison_before-480.webp 480w,/assets/img/portfolio/cnc/comparison_before-800.webp 800w,/assets/img/portfolio/cnc/comparison_before-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/cnc/comparison_before.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </img-comparison-slider> <div class="caption"> Qualitative/Quantitative comparisons of CnC against baseline models. </div> <p>By training our model <strong>Compose and Conquer (CnC)</strong> with Depth Disentanglement Training and Soft Guidance, we’ve created a model that <strong>outperforms current conditional-generation SOTA models</strong> both qualitatively and quantitatively!</p> <p><br></p> <hr> <h3 id="2-iclr-2024-noise-map-guidance-inversion-with-spatial-context-for-real-image-editing">2. [ICLR 2024] Noise Map Guidance: Inversion with Spatial Context for Real Image Editing</h3> <p><br></p> <p>Diffusion models are also refered to as Score-Based Models, due to its recent findings of how they’re also able to predict the score of random variables. These predicted scores can be utilized directly into a PF-ODE (Probability Flow ODE) that diffusion models follow: providing ways to encode/decode images without any stochasticity. This is favorable for editing real images through diffusion models: The only way to manipulate real images are to first encode them into latents that will result in the image itself after solving the PF-ODE.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/nmg/nmg-480.webp 480w,/assets/img/portfolio/nmg/nmg-800.webp 800w,/assets/img/portfolio/nmg/nmg-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/nmg/nmg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>However, there’s a caveat. Image editing methods rely on the use of Classifier-Free Guidance (CFG), which results in the reconstruction PF-ODE to diverge from its original trajectory. Past inversion methods try to overcome this by optimizing each encoded latent in the PF-ODE trajectory, which is extremely slow, even for diffusion models.</p> \[\begin{gathered} \tilde{\epsilon_\theta}\left(\boldsymbol{z}_t^{N M}, c_T\right)=\epsilon_\theta\left(\boldsymbol{z}_t^{N M}, \emptyset\right)+s_T \cdot\left(\epsilon_\theta\left(\boldsymbol{z}_t^{N M}, c_T\right)-\epsilon_\theta\left(\boldsymbol{z}_t^{N M}, \emptyset\right)\right) \\ \boldsymbol{z}_{t-1}=\sqrt{\frac{\alpha_{t-1}}{\alpha_t}} \boldsymbol{z}_t^{N M}+\sqrt{\alpha_{t-1}}\left(\sqrt{\frac{1}{\alpha_{t-1}}-1}-\sqrt{\frac{1}{\alpha_t}-1}\right) \tilde{\epsilon_\theta}\left(\boldsymbol{z}_t^{N M}, c_T\right) \end{gathered}\] <p>To overcome the slow process of optimizing each latent, we leverage the <strong>noise maps</strong>, intermediate latent representations of the PF-ODE from the timestep before in re-traversing the reconstruction PF-ODE path. As mentioned, this allows <strong>optimization-free</strong> inversion of real images, while also being robust to the structure of the original image.</p> <img-comparison-slider class="coloured-slider"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/nmg/nmg_left-480.webp 480w,/assets/img/portfolio/nmg/nmg_left-800.webp 800w,/assets/img/portfolio/nmg/nmg_left-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/nmg/nmg_left.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/nmg/nmg_right-480.webp 480w,/assets/img/portfolio/nmg/nmg_right-800.webp 800w,/assets/img/portfolio/nmg/nmg_right-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/nmg/nmg_right.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </img-comparison-slider> <div class="caption"> Qualitative/Quantitative comparisons of CnC against baseline models. </div> <p>We found that using our proposed method Noise Map Guidance (NMG), existing real image editing methods were drastically improved while being <strong>20 times faster!</strong></p> <p><br></p> <hr> <h3 id="3-cvpr-2024-one-shot-structure-aware-stylized-image-synthesis">3. [CVPR 2024] One-Shot Structure-Aware Stylized Image Synthesis</h3> <p><br></p> <p>Diffusion models are considered very attractive in the domain of stylization due to its potency and expressiveness. While GAN based stylization methods have been thoroughly explored, stylization via diffusion models are relatively under explored. GAN based stylization methods often fail in preserving rarely-seen attributes in images, and Diffusion based stylization methods require many style images in order to perform stylization (Multi-shot).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/osasis/osasis-480.webp 480w,/assets/img/portfolio/osasis/osasis-800.webp 800w,/assets/img/portfolio/osasis/osasis-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/osasis/osasis.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Note: Me and the authors of OSASIS still haven't decided on how to pronounce the model. We're going back and forth on either making the first S silent (oʊˈeɪsɪs, like wonderwall) or not (əʊˈsasɪs, like oh-SAH-sis). </div> <p>To this end, we’ve proposed One-Shot Structure-Aware Stylized Image Synthesis (OSASIS), a diffusion model capable of disentangling the structure and semantics of an image, and <strong>stylizing real images with just a single reference style.</strong> OSASIS leverages the CLIP space, and we train the model by making sure the CLIP space between four images stay parallel, respective to its counterpart. We also formulate a Structure Preserving Network (SPN) that ensures the stylized output retains the structure of the original input image.</p> <img-comparison-slider class="coloured-slider"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/osasis/osasis_left-480.webp 480w,/assets/img/portfolio/osasis/osasis_left-800.webp 800w,/assets/img/portfolio/osasis/osasis_left-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/osasis/osasis_left.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/osasis/osasis_right-480.webp 480w,/assets/img/portfolio/osasis/osasis_right-800.webp 800w,/assets/img/portfolio/osasis/osasis_right-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/osasis/osasis_right.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </img-comparison-slider> <div class="caption"> Qualitative/Quantitative comparisons of OSASIS against baseline models. </div> <p>Designed to be a <strong>training dataset-free one-shot model</strong> (only requiring a single reference image), OSASIS is robust in preserving the structure of rarely-seen attributes of a dataset!</p> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Jonghyun (Thomas) Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/index.min.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-element-bundle.min.js" integrity="sha256-BPrwikijIybg9OQC5SYFFqhBjERYOn97tCureFgYH1E=" crossorigin="anonymous"></script> </body> </html>