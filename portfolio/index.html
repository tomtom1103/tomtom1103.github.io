<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> portfolio | Jonghyun (Thomas) Lee </title> <meta name="author" content="Jonghyun (Thomas) Lee"> <meta name="description" content="In God we trust, all others bring data. - William Edwards Deming (1900-1993)"> <meta name="keywords" content="deep generative models"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/apple-touch-icon-precomposed.png?f3b0e00b51d3560daeef2cfef2a1c566"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tomtom1103.github.io/portfolio/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?ac1a8a24b4b1b97e0b04e951186c207f"></script> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/styles.min.css"> <link defer rel="stylesheet" href="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-bundle.min.css" integrity="sha256-yUoNxsvX+Vo8Trj3lZ/Y5ZBf8HlBFsB6Xwm7rH75/9E=" crossorigin="anonymous"> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jonghyun</span> (Thomas) Lee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item active"> <a class="nav-link" href="/portfolio/">portfolio <span class="sr-only">(current)</span> </a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/Tom_Resume_0228.pdf" target="_blank" rel="noopener noreferrer">cv <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">portfolio</h1> <p class="post-description">In God we trust, all others bring data. - William Edwards Deming (1900-1993)</p> </header> <article> <style>.coloured-slider{--divider-color:rgba(0,0,0,0.5);--default-handle-color:rgba(0,0,0,0.5)}</style> <hr> <h2 id="publications"><strong>Publications</strong></h2> <p><br></p> <h3 id="1-iclr-2024-compose-and-conquer-diffusion-based-3d-depth-aware-composable-image-synthesis"><strong>1. [ICLR 2024] Compose and Conquer: Diffusion-Based 3D Depth Aware Composable Image Synthesis</strong></h3> <p><strong>First Author, accepted to ICLR 2024 - 2024.01.16</strong></p> <p><br></p> <p>Conditional diffusion models usually take in text, and two different types of conditions to generate an image.</p> <ol> <li> <strong>Local conditions</strong>, which conditions the model on structural information through primitives like depth maps and canny edges.</li> <li> <strong>Global conditoins</strong>, which conditions the model on semantic information (color, identity, texture, etc).</li> </ol> <p>However, these models suffer from two limitations: they lack the ability to disentangle the relative depth of multiple local conditions, and are inable to localize global conditions to a specific area.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/cnc/1-480.webp 480w,/assets/img/portfolio/cnc/1-800.webp 800w,/assets/img/portfolio/cnc/1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/cnc/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li> <strong>Limitation 1:</strong> Suppose you’d like to generate an image of two puppies with one in front of another that have the shape of the depthmaps. Models aren’t able to distinguish that one puppy has to be placed in front of the other, resulting in a fused image.</li> <li> <strong>Limitation 2:</strong> How about an elephant standing in a forest, that shares the shape (depthmap) and semantic of the forest image? Models can’t tell that the image semantics are supposed to go behind (or around) the elephant, resulting in the elephant being completely ignored.</li> </ul> <p>We sought to create a model that was able to <strong>1. Distinguish between where objects should be placed in relative depths, and 2. Localize global semantics onto a user-defined area</strong>, not just the whole image.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/cnc/2-480.webp 480w,/assets/img/portfolio/cnc/2-800.webp 800w,/assets/img/portfolio/cnc/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/cnc/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We solved this through proposing a training paradigm and a novel inference algorithm: <strong>Depth Disentanglement Training</strong> and <strong>Soft Guidance</strong>.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/cnc/ddt-480.webp 480w,/assets/img/portfolio/cnc/ddt-800.webp 800w,/assets/img/portfolio/cnc/ddt-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/cnc/ddt.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>Depth Disentanglement Training</strong> parses the training data into three sub-components: the foreground, background, and mask via inpainting. The background image serves as structural information that’s initially occluded by the salient object, forcing the model to learn what’s <strong>behind</strong> another object.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/cnc/soft_guidance-480.webp 480w,/assets/img/portfolio/cnc/soft_guidance-800.webp 800w,/assets/img/portfolio/cnc/soft_guidance-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/cnc/soft_guidance.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>Soft Guidance</strong> partially masks out the pixels that are attending to certain semantics in the cross-attention layers of the model, forcing the model to attend to <strong>parts of the pixels</strong> that correspond to their semantic counterparts.</p> <img-comparison-slider class="coloured-slider"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/cnc/comparison_after-480.webp 480w,/assets/img/portfolio/cnc/comparison_after-800.webp 800w,/assets/img/portfolio/cnc/comparison_after-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/cnc/comparison_after.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/cnc/comparison_before-480.webp 480w,/assets/img/portfolio/cnc/comparison_before-800.webp 800w,/assets/img/portfolio/cnc/comparison_before-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/cnc/comparison_before.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </img-comparison-slider> <div class="caption"> Qualitative/Quantitative comparisons of CnC against baseline models. </div> <p>By training our model <strong>Compose and Conquer (CnC)</strong> with Depth Disentanglement Training and Soft Guidance, we’ve created a model that <strong>outperforms current conditional-generation SOTA models</strong> both qualitatively and quantitatively!</p> <p><a href="https://arxiv.org/abs/2401.09048" rel="external nofollow noopener" target="_blank">Paper link</a>, <a href="https://github.com/tomtom1103/compose-and-conquer/" rel="external nofollow noopener" target="_blank">Code link</a></p> <p>Tools used: <a href=""><img src="https://img.shields.io/badge/PyTorch-white?style=flat-square&amp;logo=Pytorch" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Docker-white?style=flat-square&amp;logo=Docker" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Bash-white?style=flat-square&amp;logo=GNU%20Bash" alt=""></a> <a href=""><img src="https://img.shields.io/badge/CUDA-white?style=flat-square&amp;logo=NVIDIA" alt=""></a> <a href=""><img src="https://img.shields.io/badge/OpenCV-5C3EE8?style=flat-square&amp;logo=OpenCV" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Git-white?style=flat-square&amp;logo=Git" alt=""></a> <a href=""><img src="https://img.shields.io/badge/NumPy-013243?style=flat-square&amp;logo=NumPy" alt=""></a></p> <p><br></p> <hr> <h3 id="2-iclr-2024-noise-map-guidance-inversion-with-spatial-context-for-real-image-editing"><strong>2. [ICLR 2024] Noise Map Guidance: Inversion with Spatial Context for Real Image Editing</strong></h3> <p><strong>Second Author, accepted to ICLR 2024 - 2024.01.16</strong></p> <p><br></p> <p>Diffusion models are also refered to as Score-Based Models, due to its recent findings of how they’re also able to predict the score of random variables. These predicted scores can be utilized directly into a PF-ODE (Probability Flow ODE) that diffusion models follow: providing ways to encode/decode images without any stochasticity. This is favorable for editing real images through diffusion models: The only way to manipulate real images are to first encode them into latents that will result in the image itself after solving the PF-ODE.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/nmg/nmg-480.webp 480w,/assets/img/portfolio/nmg/nmg-800.webp 800w,/assets/img/portfolio/nmg/nmg-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/nmg/nmg.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>However, there’s a caveat. Image editing methods rely on the use of Classifier-Free Guidance (CFG), which results in the reconstruction PF-ODE to diverge from its original trajectory. Past inversion methods try to overcome this by optimizing each encoded latent in the PF-ODE trajectory, which is extremely slow, even for diffusion models.</p> \[\begin{gathered} \tilde{\epsilon_\theta}\left(\boldsymbol{z}_t^{N M}, c_T\right)=\epsilon_\theta\left(\boldsymbol{z}_t^{N M}, \emptyset\right)+s_T \cdot\left(\epsilon_\theta\left(\boldsymbol{z}_t^{N M}, c_T\right)-\epsilon_\theta\left(\boldsymbol{z}_t^{N M}, \emptyset\right)\right) \\ \boldsymbol{z}_{t-1}=\sqrt{\frac{\alpha_{t-1}}{\alpha_t}} \boldsymbol{z}_t^{N M}+\sqrt{\alpha_{t-1}}\left(\sqrt{\frac{1}{\alpha_{t-1}}-1}-\sqrt{\frac{1}{\alpha_t}-1}\right) \tilde{\epsilon_\theta}\left(\boldsymbol{z}_t^{N M}, c_T\right) \end{gathered}\] <p>To overcome the slow process of optimizing each latent, we leverage the <strong>noise maps</strong>, intermediate latent representations of the PF-ODE from the timestep before in re-traversing the reconstruction PF-ODE path. As mentioned, this allows <strong>optimization-free</strong> inversion of real images, while also being robust to the structure of the original image.</p> <img-comparison-slider class="coloured-slider"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/nmg/nmg_left-480.webp 480w,/assets/img/portfolio/nmg/nmg_left-800.webp 800w,/assets/img/portfolio/nmg/nmg_left-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/nmg/nmg_left.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/nmg/nmg_right-480.webp 480w,/assets/img/portfolio/nmg/nmg_right-800.webp 800w,/assets/img/portfolio/nmg/nmg_right-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/nmg/nmg_right.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </img-comparison-slider> <div class="caption"> Qualitative/Quantitative comparisons of CnC against baseline models. </div> <p>We found that using our proposed method Noise Map Guidance (NMG), existing real image editing methods were drastically improved while being <strong>20 times faster!</strong></p> <p><a href="https://arxiv.org/abs/2402.04625" rel="external nofollow noopener" target="_blank">Paper link</a>, <a href="https://github.com/hansam95/NMG" rel="external nofollow noopener" target="_blank">Code link</a></p> <p>Tools used: <a href=""><img src="https://img.shields.io/badge/PyTorch-white?style=flat-square&amp;logo=Pytorch" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Docker-white?style=flat-square&amp;logo=Docker" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Bash-white?style=flat-square&amp;logo=GNU%20Bash" alt=""></a> <a href=""><img src="https://img.shields.io/badge/CUDA-white?style=flat-square&amp;logo=NVIDIA" alt=""></a> <a href=""><img src="https://img.shields.io/badge/OpenCV-5C3EE8?style=flat-square&amp;logo=OpenCV" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Git-white?style=flat-square&amp;logo=Git" alt=""></a> <a href=""><img src="https://img.shields.io/badge/NumPy-013243?style=flat-square&amp;logo=NumPy" alt=""></a></p> <p><br></p> <hr> <h3 id="3-cvpr-2024-one-shot-structure-aware-stylized-image-synthesis"><strong>3. [CVPR 2024] One-Shot Structure-Aware Stylized Image Synthesis</strong></h3> <p><strong>Second Author, accepted to CVPR 2024 - 2024.02.27</strong></p> <p><br></p> <p>Diffusion models are considered very attractive in the domain of stylization due to its potency and expressiveness. While GAN based stylization methods have been thoroughly explored, stylization via diffusion models are relatively under explored. GAN based stylization methods often fail in preserving rarely-seen attributes in images, and Diffusion based stylization methods require many style images in order to perform stylization (Multi-shot).</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/osasis/osasis-480.webp 480w,/assets/img/portfolio/osasis/osasis-800.webp 800w,/assets/img/portfolio/osasis/osasis-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/osasis/osasis.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="caption"> Note: Me and the authors of OSASIS still haven't decided on how to pronounce the model. We're going back and forth on either making the first S silent (oʊˈeɪsɪs, like wonderwall) or not (əʊˈsasɪs, like oh-SAH-sis). </div> <p>To this end, we’ve proposed One-Shot Structure-Aware Stylized Image Synthesis (OSASIS), a diffusion model capable of disentangling the structure and semantics of an image, and <strong>stylizing real images with just a single reference style.</strong> OSASIS leverages the CLIP space, and we train the model by making sure the CLIP space between four images stay parallel, respective to its counterpart. We also formulate a Structure Preserving Network (SPN) that ensures the stylized output retains the structure of the original input image.</p> <img-comparison-slider class="coloured-slider"> <figure slot="first"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/osasis/osasis_left-480.webp 480w,/assets/img/portfolio/osasis/osasis_left-800.webp 800w,/assets/img/portfolio/osasis/osasis_left-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/osasis/osasis_left.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <figure slot="second"> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/osasis/osasis_right-480.webp 480w,/assets/img/portfolio/osasis/osasis_right-800.webp 800w,/assets/img/portfolio/osasis/osasis_right-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/osasis/osasis_right.jpg" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </img-comparison-slider> <div class="caption"> Qualitative/Quantitative comparisons of OSASIS against baseline models. </div> <p>Designed to be a <strong>training dataset-free one-shot model</strong> (only requiring a single reference image), OSASIS is robust in preserving the structure of rarely-seen attributes of a dataset!</p> <p><a href="https://arxiv.org/abs/2402.17275" rel="external nofollow noopener" target="_blank">Paper link</a> [Code coming soon!]</p> <p>Tools used: <a href=""><img src="https://img.shields.io/badge/PyTorch-white?style=flat-square&amp;logo=Pytorch" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Docker-white?style=flat-square&amp;logo=Docker" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Bash-white?style=flat-square&amp;logo=GNU%20Bash" alt=""></a> <a href=""><img src="https://img.shields.io/badge/CUDA-white?style=flat-square&amp;logo=NVIDIA" alt=""></a> <a href=""><img src="https://img.shields.io/badge/OpenCV-5C3EE8?style=flat-square&amp;logo=OpenCV" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Git-white?style=flat-square&amp;logo=Git" alt=""></a> <a href=""><img src="https://img.shields.io/badge/NumPy-013243?style=flat-square&amp;logo=NumPy" alt=""></a></p> <p><br></p> <hr> <h2 id="projects"><strong>Projects</strong></h2> <p><br></p> <h3 id="1-ai-spark-challenge---irdis-immediate-rescue-based-disaster-response-system"><strong>1. AI Spark Challenge - IRDIS (Immediate Rescue Based Disaster Response System)</strong></h3> <p><strong>Ministry of Science and ICT lead AI Tournament, Final Selection - 2022.04</strong></p> <p><br></p> <p>For the hackathon, I lead team <strong>IRDIS</strong> (Immediate Rescue Based Disaster Response System), where we were tasked to develp a disaster response solution.</p> <p>We decided to utilize the <a href="https://xview2.org/dataset" rel="external nofollow noopener" target="_blank">xView2 Dataset</a>, containing 22K satellite images. Each image has a pre/post disaster counterpart, and labels containing information about what disaster had occured, the level of damage, and segmentation labels for roads and buildings.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/irdis/2-480.webp 480w,/assets/img/portfolio/irdis/2-800.webp 800w,/assets/img/portfolio/irdis/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/irdis/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We first trained two seperate UNet models on the xView2 Dataset on the task of semantic segmentation, each detecting the segmentation maps for buildings and roads. Then, we trained a third model conditioned on the type of building and segmentation map to predict the damage level for a building.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/irdis/3-480.webp 480w,/assets/img/portfolio/irdis/3-800.webp 800w,/assets/img/portfolio/irdis/3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/irdis/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We then created a Scoring/Ranking system based off the Analytical Hierarchy Process (AHP), which ranked which building was in need of immediate rescue. The criterions of the AHP took in the type of building, how many buildings were in proximity, how badly damaged the buildings were, the time of day, and what kind of road lead up to said building.</p> <p><a href="https://github.com/tomtom1103/AISpark_Challenge_IRDIS" rel="external nofollow noopener" target="_blank">Code link</a></p> <p>Tools used: <a href=""><img src="https://img.shields.io/badge/PyTorch-white?style=flat-square&amp;logo=Pytorch" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Pandas-150458?style=flat-square&amp;logo=Pandas" alt=""></a> <a href=""><img src="https://img.shields.io/badge/GeoPandas-139C5A?style=flat-square&amp;logo=GeoPandas" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Bash-white?style=flat-square&amp;logo=GNU%20Bash" alt=""></a> <a href=""><img src="https://img.shields.io/badge/CUDA-white?style=flat-square&amp;logo=NVIDIA" alt=""></a> <a href=""><img src="https://img.shields.io/badge/OpenCV-5C3EE8?style=flat-square&amp;logo=OpenCV" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Git-white?style=flat-square&amp;logo=Git" alt=""></a> <a href=""><img src="https://img.shields.io/badge/NumPy-013243?style=flat-square&amp;logo=NumPy" alt=""></a></p> <p><br></p> <h3 id="2-kuiai-hackathon---multi-polygon-map-based-business-location-recommendation-system"><strong>2. KUIAI Hackathon - Multi-polygon map based business location recommendation system</strong></h3> <p><strong>Korea Electronics Technology Institute (KETI) lead AI Hackathon, 3rd place/20 teams - 2022.01</strong></p> <p><br></p> <p>For the hackathon, I lead team <strong>Journey Lee</strong>, where we were tasked to deduct meaningful data collected from commercial stores/buildings across Seoul, South Korea in 3 days. We decided to build a location recommendation system for the commercial success of businesses.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/journeylee/1-480.webp 480w,/assets/img/portfolio/journeylee/1-800.webp 800w,/assets/img/portfolio/journeylee/1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/journeylee/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We trained a simple model with 8 FC layers, tasked to predict the quarterly sales of a business based on multiple variables, including, but not limited to: geographic coordinates, type of business, sales per day of the week, sales per age group. We then created an online tool that would take in the type of business and geographic coordinate, and output the predicted monthly sale of said location and any other viable address within a 500 meter radius.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/journeylee/2-480.webp 480w,/assets/img/portfolio/journeylee/2-800.webp 800w,/assets/img/portfolio/journeylee/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/journeylee/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>On the online tool, we’ve also included a GeoPandas based EDA tool, where data about the various variables used to train the model are visualized.</p> <p><a href="https://github.com/tomtom1103/KUIAI_Hackathon_2022/tree/main" rel="external nofollow noopener" target="_blank">Code link</a>, <a href="https://tomtom1103-kuiai-hackathon-2022-jl-app-6kr5kv.streamlit.app/" rel="external nofollow noopener" target="_blank">Web-App link</a></p> <p>Tools used: <a href=""><img src="https://img.shields.io/badge/PyTorch-white?style=flat-square&amp;logo=Pytorch" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Streamlit-white?style=flat-square&amp;logo=Streamlit" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Pandas-150458?style=flat-square&amp;logo=Pandas" alt=""></a> <a href=""><img src="https://img.shields.io/badge/GeoPandas-139C5A?style=flat-square&amp;logo=GeoPandas" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Bash-white?style=flat-square&amp;logo=GNU%20Bash" alt=""></a> <a href=""><img src="https://img.shields.io/badge/CUDA-white?style=flat-square&amp;logo=NVIDIA" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Git-white?style=flat-square&amp;logo=Git" alt=""></a> <a href=""><img src="https://img.shields.io/badge/NumPy-013243?style=flat-square&amp;logo=NumPy" alt=""></a></p> <p><br></p> <h3 id="3-k-data-datacampus---audio-image-korean-phoneme-recognition-model"><strong>3. K-Data Datacampus - Audio-Image Korean phoneme recognition model</strong></h3> <p><strong>Korea Data Agency lead AI Tournament, 2nd place/40 teams - 2021.09</strong></p> <p>For the tournament, I lead team <strong>Gillajab-i</strong>, where we first developed and trained a transformer based ASR Korean phoneme recognition model, and serviced a web-app for Korean word pronunciation error, measured by a Levenshtein distance based character-error rate (CER) metric.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/gillajabi/1-480.webp 480w,/assets/img/portfolio/gillajabi/1-800.webp 800w,/assets/img/portfolio/gillajabi/1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/gillajabi/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We collected 1000 hours of raw Korean speaking data, re-tokenized the ground truth speech to phonemes, and trained a ViT. The model took in audio, converted it to Mel-Spectrograms, and predicted the phonemes (instead of words) from said audio clip, reaching a test accuracy of 93%.</p> <p>Targeted at foreigners learning to speak korean, the web-app would follow these steps:</p> <ol> <li>A user would choose or upload a short video of someone saying the phrase they would want to learn</li> <li>Audio is extracted from the video</li> <li>Model converts the audio into Korean text</li> <li>Converts the Korean text into Romaji (for the user to read)</li> <li>Converts the Korean text into English (for the user to understand)</li> </ol> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/gillajabi/2-480.webp 480w,/assets/img/portfolio/gillajabi/2-800.webp 800w,/assets/img/portfolio/gillajabi/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/gillajabi/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>The user would record themselves practicing the pronunciation, and our ViT would inference the phonemes from the speech, and calculate the error rate via Levenshtein distance.</p> <p><a href="https://github.com/tomtom1103/DataYouthCampus-Team4" rel="external nofollow noopener" target="_blank">Code link</a></p> <p>Tools used: <a href=""><img src="https://img.shields.io/badge/PyTorch-white?style=flat-square&amp;logo=Pytorch" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Streamlit-white?style=flat-square&amp;logo=Streamlit" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Bash-white?style=flat-square&amp;logo=GNU%20Bash" alt=""></a> <a href=""><img src="https://img.shields.io/badge/CUDA-white?style=flat-square&amp;logo=NVIDIA" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Git-white?style=flat-square&amp;logo=Git" alt=""></a> <a href=""><img src="https://img.shields.io/badge/NumPy-013243?style=flat-square&amp;logo=NumPy" alt=""></a></p> <p><br></p> <h3 id="4-smart-campus-datathon---unsupervised-embedding-model-based-scholarship-parsingrecommendation-system"><strong>4. Smart Campus Datathon - Unsupervised embedding model based scholarship parsing/recommendation system</strong></h3> <p><strong>Korea University Datahub lead AI Datathon, 2nd place/15 teams - 2021.09</strong></p> <p><br></p> <p>For the datathon, I lead team <strong>Peachtree</strong>, where we we developed a web-app with an embedded unsupervised embedding model that parsed and recommended scholarships based off a user’s age, major, GPA, home address, previous scholarship grants, and scholarship eligibility.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/peachtree/engine-480.webp 480w,/assets/img/portfolio/peachtree/engine-800.webp 800w,/assets/img/portfolio/peachtree/engine-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/peachtree/engine.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/portfolio/peachtree/fe-480.webp 480w,/assets/img/portfolio/peachtree/fe-800.webp 800w,/assets/img/portfolio/peachtree/fe-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/portfolio/peachtree/fe.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>We first crawled data from major scholarship hosting platforms targeted to undergraduate students, and preprocessed the unstructured data. We then used Doc2Vec as an embedding model to embed all the scholarships, and used DBSCAN to cluster said scholarships. For the actual recommendation of scholarships, ‘anchor’ students were matched with a cluster based on their previous scholarship grants, and the cosine similarity between an inference student and all the ‘anchor’ students were calculated to place the inference student in a cluster. A final rule based algorithm filters out scholarships that aren’t eligible, and students were recommended scholarships that were left in the cluster.</p> <p><a href="https://github.com/tomtom1103/2021Datathon_Peachtree" rel="external nofollow noopener" target="_blank">Code link</a></p> <p>Tools used: <a href=""><img src="https://img.shields.io/badge/sklearn-white?style=flat-square&amp;logo=scikitlearn" alt=""></a> <a href=""><img src="https://img.shields.io/badge/NumPy-013243?style=flat-square&amp;logo=NumPy" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Pandas-150458?style=flat-square&amp;logo=Pandas" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Django-092E20?style=flat-square&amp;logo=Django" alt=""></a> <a href=""><img src="https://img.shields.io/badge/MariaDB-003545?style=flat-square&amp;logo=MariaDB" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Bash-white?style=flat-square&amp;logo=GNU%20Bash" alt=""></a> <a href=""><img src="https://img.shields.io/badge/Git-white?style=flat-square&amp;logo=Git" alt=""></a></p> <p><br></p> </article> </div> </div> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script defer src="https://cdn.jsdelivr.net/npm/img-comparison-slider@8.0.6/dist/index.min.js"></script> <script defer src="https://cdn.jsdelivr.net/npm/swiper@11.0.5/swiper-element-bundle.min.js" integrity="sha256-BPrwikijIybg9OQC5SYFFqhBjERYOn97tCureFgYH1E=" crossorigin="anonymous"></script> </body> </html>