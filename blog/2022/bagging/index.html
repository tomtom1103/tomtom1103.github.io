<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Ensemble Learning - Bagging and the Random Forest | Jonghyun (Thomas) Lee </title> <meta name="author" content="Jonghyun (Thomas) Lee"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="deep generative models"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/apple-touch-icon-precomposed.png?f3b0e00b51d3560daeef2cfef2a1c566"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tomtom1103.github.io/blog/2022/bagging/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jonghyun</span> (Thomas) Lee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Ensemble Learning - Bagging and the Random Forest</h1> <p class="post-meta"> July 05, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="contents">Contents</h2> <ul> <li>Ensemble Learning</li> <li>Bootstrap Aggregating: Bagging <ul> <li>Bootstraps</li> <li>Result Aggregating <ul> <li>Majority Voting</li> <li>Weighted Voting: (Validation Accuracy)</li> <li>Weighted Voting: (Predicted Probability for each class)</li> <li>Stacking</li> </ul> </li> </ul> </li> <li>Random Forests <ul> <li>Variable Importance: OOB</li> </ul> </li> </ul> <blockquote> <p>해당 포스트는 2021년 1학기 고려대학교 강필성 교수님의 ‘<a href="https://github.com/pilsung-kang/multivariate-data-analysis" rel="external nofollow noopener" target="_blank">다변량분석</a>’ 강의를 참고하여 작성되었습니다.</p> <p>이미지는 강필성 교수님의 강의 슬라이드에서 발췌하였습니다.</p> <p><del>A+ 감사했습니다 교수님</del></p> </blockquote> <h2 id="ensemble-learning">Ensemble Learning</h2> <p>이전 장에서 Bias-Variance Tradeoff 에 대해 알아보았다. 머신러닝 알고리즘의 model complexity 가 높다면 보통 Low Bias High Variance 의 특징을 가지며, model complexity 가 낮다면 High Bias Low Variance 의 특징을 갖는다. 둘 중 하나는 본질적으로 높기 때문에, 높은 값을 낮게 만들어주는 앙상블 학습은 크게 Bagging 과 Boosting, 두가지 방법론으로 나뉜다.</p> <p>Bagging: Decreases Variance</p> <p>Boosting: Decreases Bias</p> <p>앙상블 학습의 핵심은 같은 분포에서 추출된 \(N\)개의 데이터셋을 가지고 \(N\)개의 모델을 학습 시킨 뒤, 이 모델들을 적절히 결합하는 것이다.</p> <h2 id="bootstrap-aggregating-bagging">Bootstrap Aggregating: Bagging</h2> <h3 id="bootstraps">Bootstraps</h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/machinelearning/bagging/img1-480.webp 480w,/assets/img/posts/machinelearning/bagging/img1-800.webp 800w,/assets/img/posts/machinelearning/bagging/img1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/machinelearning/bagging/img1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Bagging 은 특정 모델의 분산을 낮춰주는 학습 방법론이다. 위 그림과 같이 기존 데이터셋은 10개의 샘플을 가지고 있다. 이때 기존 데이터셋에서 10개의 샘플을 random 으로 <strong>복원추출</strong>을 하여 \(B\) 개의 새로운 데이터셋, 즉 Bootstrap 을 생성한다. 해당 Bootstrap 들로 동일한 알고리즘을 학습 한 뒤, 결과를 하나로 합치는 것이 Bagging 의 핵심이다. 이때 알고리즘은 지도학습 알고리즘이면 어떤 것을 사용해도 무방하지만, Bagging 은 앞서 언급하였듯이 high model complexity 를 가진 알고리즘들에 효과적이다. Bagging 자체는 알고리즘이 아니라 앙상블의 다양성을 확보하기 위한 방법론 중 하나이기 때문에, 어떤 알고리즘을 사용하냐에 따라 Bagging with Neural Networks, Bagging with SVM 등으로 불리는 명칭이 달라진다.</p> <h3 id="result-aggregating">Result Aggregating</h3> <p>\(B\) 개의 Bootstrap 으로 \(B\) 개의 모델을 학습시켰다면, 최종적으로 이 모델들을 하나로 합쳐야 한다. Bagging 에서 이 단계를 Result Aggregating 이라 하며, 다양한 방법론들이 있다.</p> <blockquote> <p>개인적으로 앙상블에서 ‘모델을 Aggregate 한다’ 라는 표현을 싫어한다. 모델을 합치는 단계에선 Classification 하고자 하는 새로운 sample (혹은 test data) 에 대해 각 모델이 각자 예측을 진행하고, 이 결과들을 특정 가중합으로 합쳐 최종 예측값을 뱉어낸다. Black Box 인 모델 ‘자체’ 를 합치는 것이 아니라 개별 모델의 ‘output’ 을 합치는 것이다.</p> </blockquote> <h4 id="majority-voting">Majority Voting</h4> <p>각 모델의 결과를 합치는 방법론 중 가장 간단한 방법론인 Majority Voting 은, 과반수의 예측값을 최종 예측값으로 선택하는 것이다.</p> \[\hat y_{Ensemble}=argmax(\sum_{j=1}^n\delta(\hat y_j=i), \ i \in \{0,1\})\] <table> <thead> <tr> <th>Ensemble Population</th> <th>Validation Accuracy</th> <th>P(y=1) for a test sample</th> <th>Predicted Class Label</th> </tr> </thead> <tbody> <tr> <td>Model 1</td> <td>0.80</td> <td>0.90</td> <td><strong>1</strong></td> </tr> <tr> <td>Model 2</td> <td>0.75</td> <td>0.92</td> <td><strong>1</strong></td> </tr> <tr> <td>Model 3</td> <td>0.88</td> <td>0.87</td> <td><strong>1</strong></td> </tr> <tr> <td>Model 4</td> <td>0.91</td> <td>0.34</td> <td><strong>0</strong></td> </tr> <tr> <td>Model 5</td> <td>0.77</td> <td>0.41</td> <td><strong>0</strong></td> </tr> <tr> <td>Model 6</td> <td>0.65</td> <td>0.84</td> <td><strong>1</strong></td> </tr> <tr> <td>Model 7</td> <td>0.95</td> <td>0.14</td> <td><strong>0</strong></td> </tr> <tr> <td>Model 8</td> <td>0.82</td> <td>0.32</td> <td><strong>0</strong></td> </tr> <tr> <td>Model 9</td> <td>0.78</td> <td>0.98</td> <td><strong>1</strong></td> </tr> <tr> <td>Model 10</td> <td>0.83</td> <td>0.57</td> <td><strong>1</strong></td> </tr> </tbody> </table> <p>위의 예시는 10개의 Bootstrap 을 통해 10개의 Classification 모델을 학습시킨 예시다. 각 모델은 새로 들어온 test sample 에 대해 class 1 에 포함될 확률과, 0.5 로 설정된 cutoff 로 sample 에 대한 최종 예측 class 를 보여준다. 이때 Majority Voting 은 단순히 10개의 모델 중 해당 샘플이 class 1 이라고 예측한 모델의 수가 더 많기 때문에, 해당 샘플에 대한 앙상블의 최종 예측값은 1이 된다.</p> \[\sum^n_{j=1}\delta (\hat y_j=0) = 4 \\ \sum^n_{j=1}\delta (\hat y_j=1) = 6 \\ \hat y_{Ensemble}=1\] <h4 id="weighted-voting-validation-accuracy">Weighted Voting (Validation Accuracy)</h4> <p>각 모델의 결과에 가중치를 부여해서 합칠 수 있는데, 가중치를 각 모델의 Validation Accuracy 로 설정한 예시는 다음과 같다.</p> \[\hat y_{Ensemble}=argmax \left( {\sum_{j=1}^n(ValAcc_j) \cdot \delta(\hat y_j=i) \over \sum_{j=1}^n(ValAcc_j)}, \ i \in \{0,1\} \right)\] <table> <thead> <tr> <th>Ensemble Population</th> <th>Validation Accuracy</th> <th>P(y=1) for a test sample</th> <th>Predicted Class Label</th> </tr> </thead> <tbody> <tr> <td>Model 1</td> <td><strong>0.80</strong></td> <td>0.90</td> <td><strong>1</strong></td> </tr> <tr> <td>Model 2</td> <td><strong>0.75</strong></td> <td>0.92</td> <td><strong>1</strong></td> </tr> <tr> <td>Model 3</td> <td><strong>0.88</strong></td> <td>0.87</td> <td><strong>1</strong></td> </tr> <tr> <td>Model 4</td> <td><strong>0.91</strong></td> <td>0.34</td> <td><strong>0</strong></td> </tr> <tr> <td>Model 5</td> <td><strong>0.77</strong></td> <td>0.41</td> <td><strong>0</strong></td> </tr> <tr> <td>Model 6</td> <td><strong>0.65</strong></td> <td>0.84</td> <td><strong>1</strong></td> </tr> <tr> <td>Model 7</td> <td><strong>0.95</strong></td> <td>0.14</td> <td><strong>0</strong></td> </tr> <tr> <td>Model 8</td> <td><strong>0.82</strong></td> <td>0.32</td> <td><strong>0</strong></td> </tr> <tr> <td>Model 9</td> <td><strong>0.78</strong></td> <td>0.98</td> <td><strong>1</strong></td> </tr> <tr> <td>Model 10</td> <td><strong>0.83</strong></td> <td>0.57</td> <td><strong>1</strong></td> </tr> </tbody> </table> <p>각 모델의 Validation Accuracy 를 \(\delta\) class label 와 곱해준 뒤, Validation Accuracy 의 합으로 나눠줌으로써 정규화를 진행해준다. 이때 \(\delta\) 는 단순한 counting function 이다. 높은 Validation Accuracy 는 모델의 신뢰도가 높다는 것을 의미하기 때문에, 이를 가중치로 반영한 방법론이다.</p> \[{\sum_{j=1}^n(ValAcc_j) \cdot \delta(\hat y_j=0) \over \sum_{j=1}^n(ValAcc_j)} = 0.424 \\ \\ {\sum_{j=1}^n(ValAcc_j) \cdot \delta(\hat y_j=1) \over \sum_{j=1}^n(ValAcc_j)} = 0.576 \\ \\ \hat y_{Ensemble} = 1\] <h4 id="weighted-voting-predicted-probability-for-each-class">Weighted Voting (Predicted Probability for each class)</h4> <p>가중치를 각 모델의 클래스 예측확률로 사용한 예시는 다음과 같다.</p> \[\hat y_{Ensemble}=argmax \left({1 \over n} \sum_{j=1}^n P(y=i), \ i \in \{0,1\} \right)\] <table> <thead> <tr> <th>Ensemble Population</th> <th>Validation Accuracy</th> <th>P(y=1) for a test sample</th> <th>Predicted Class Label</th> </tr> </thead> <tbody> <tr> <td>Model 1</td> <td>0.80</td> <td>0.90</td> <td><strong>1</strong></td> </tr> <tr> <td>Model 2</td> <td>0.75</td> <td>0.92</td> <td><strong>1</strong></td> </tr> <tr> <td>Model 3</td> <td>0.88</td> <td>0.87</td> <td><strong>1</strong></td> </tr> <tr> <td>Model 4</td> <td>0.91</td> <td>0.34</td> <td><strong>0</strong></td> </tr> <tr> <td>Model 5</td> <td>0.77</td> <td>0.41</td> <td><strong>0</strong></td> </tr> <tr> <td>Model 6</td> <td>0.65</td> <td>0.84</td> <td><strong>1</strong></td> </tr> <tr> <td>Model 7</td> <td>0.95</td> <td>0.14</td> <td><strong>0</strong></td> </tr> <tr> <td>Model 8</td> <td>0.82</td> <td>0.32</td> <td><strong>0</strong></td> </tr> <tr> <td>Model 9</td> <td>0.78</td> <td>0.98</td> <td><strong>1</strong></td> </tr> <tr> <td>Model 10</td> <td>0.83</td> <td>0.57</td> <td><strong>1</strong></td> </tr> </tbody> </table> \[{1 \over n} \sum_{j=1}^n P(y=0) = 0.375 \\ \\ {1 \over n} \sum_{j=1}^n P(y=1) = 0.625 \\ \\ \hat y_{Ensemble} = 1\] <p>앞선 예시들과 같이 개별 모델을 설명하는 값을 가중치로 적절히 이용할 수 있다. Validation Accuracy 와 각 클래스 예측확률을 적절히 조합한 가중치 또한 가능하다.</p> <h4 id="stacking">Stacking</h4> <p>Result Aggregating 중 가장 높은 성능을 내는 방법론 중 하나인 Stacking 은 앞선 weighted voting 과는 조금 다른 approach 를 이용한다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/machinelearning/bagging/img2-480.webp 480w,/assets/img/posts/machinelearning/bagging/img2-800.webp 800w,/assets/img/posts/machinelearning/bagging/img2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/machinelearning/bagging/img2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Bootstrap 으로 각 모델을 생성한 뒤, 새로운 샘플에 대한 예측치를 다시 입력치 \(\mathbf{x}\) 로, 실제 정답을 \(y\) 로 갖는 새로운 Meta-Classifier 을 학습하는 방법론이다.</p> <h2 id="random-forests">Random Forests</h2> <p>사실 Bagging 은 굉장히 직관적인 앙상블 기법이다. 복원추출로 동일한 샘플수를 가진 bootstrap 을 생성하고, 모델들을 학습시킨 뒤 적절히 결합하여 complex 모델의 분산을 줄인다. High complexity 를 가진 그 어떤 모델에 bagging 을 적용할 수 있고, base learner (기준이 되는 학습 알고리즘) 에 따라 bagging with SVM 등으로 불린다. 반면 Random Forest 는 의사결정나무에 bagging 을 적용한 모델로, 특별히 혼자 다르게 불리는 이유는 OOB 데이터셋에 의한 추가적인 insight 도출 능력과 bagging 에 있어 모든 설명변수를 사용하지 않는다는 특징이 있기 때문이다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/machinelearning/bagging/img3-480.webp 480w,/assets/img/posts/machinelearning/bagging/img3-800.webp 800w,/assets/img/posts/machinelearning/bagging/img3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/machinelearning/bagging/img3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Random Forest 는 이름에서 알 수 있듯이 의사결정나무의 앙상블 모델이다. 기존 Bagging 을 사용하지만, Bootstrap 을 이용한 학습 단계에서 모든 설명변수를 사용하지 않고 random 한 설명변수의 subset 만으로 학습을 진행한다는 특징을 가지고 있다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/machinelearning/bagging/img4-480.webp 480w,/assets/img/posts/machinelearning/bagging/img4-800.webp 800w,/assets/img/posts/machinelearning/bagging/img4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/machinelearning/bagging/img4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>위 예시는 25개의 설명변수를 가진 데이터셋에서 Bagging 을 진행 한 뒤, 학습을 진행했을 때 사용된 설명변수를 나타낸다. 첫번째 split 에선 \(x_2, x_{10},x_{23}, x_{17}, x_{9}\) 설명변수들로만 split 을 진행했고, 그다음 split 에선 또 다른 설명변수의 random subset 이 사용된 걸 볼 수 있다. <strong>이러한 과정으로 통해 Random Forest 는 Bagging 으로 다양성 확보를 하고, 설명변수를 random 하게 추출함으로써 다양성을 추가적으로 더 확보할 수 있다.</strong></p> <blockquote> <p>Split 을 할때 설명변수를 random 하게 추출하지 않고 때 전부 사용한다면 만들어진 bagging tree 들은 성능이 서로 비슷하다. 하지만 random 추출을 했기 때문에 만들어진 tree 들은 성능이 서로 굉장히 다르다. 비록 개별 성능은 설명변수를 전부 사용한 개별 tree 들 보다 떨어지지만, 합치면 다양성을 보다 확보하면서 성능까지 챙길 수 있다.</p> <p>강필성 교수님께선 이를 ‘2보 전진을 위한 1보 후퇴’ 라고 표현하셨으며, Random Forest 를 관통하는 개념이다.</p> </blockquote> <h3 id="variable-importance-oob">Variable Importance: OOB</h3> <p>K-fold data split 과는 달리 Bagging 은 각 bootstrap 을 만들때 복원추출을 한다. 그렇기에 bootstrap 수가 아무리 많아도, <strong>확률적으로 단 한번도 bootstrap 에 포함되지 않는 샘플들이 있기 마련이다.</strong> 이를 OOB Data (Out-of-bag) 라고 하며, 앙상블의 개별 모델의 Validation Set 으로 쓰인다. 통상적인 머신러닝 모델을 학습시킬 때 데이터셋을 Train/Validation/Test set 으로 나누어 학습을 진행하지만, OOB의 존재 덕분에 따로 Validation Set 을 빼놓지 않고 보다 더 많은 Training sample 을 쓸 수 있다는 장점이 있다.</p> <p>앙상블 모델중에서 Random Forest 는 실무에서 활용도가 굉장히 높은 모델이다. <strong>바로 OOB Data 로 설명변수의 중요도를 구할 수 있기 때문이다.</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/machinelearning/bagging/img5-480.webp 480w,/assets/img/posts/machinelearning/bagging/img5-800.webp 800w,/assets/img/posts/machinelearning/bagging/img5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/machinelearning/bagging/img5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p><strong>Step 1:</strong> Population (앙상블을 구성하는 개별 모델) 에 대해 OOB error (\(e_i\)) 를 계산한다.</p> <p><strong>Step 2:</strong> 중요도를 구하고자 하는 설명변수 \(x_i\) 의 값들에 대해 permutation 을 진행 후, permutated OOB error (\(p_i\)) 를 계산한다.</p> <p><strong>Step 3:</strong> Population 에 대해 \(p_i - e_i\) 의 평균과 분산을 구한 뒤, 변수의 중요도를 산출한다.</p> <p>이때 Step 2 의 permutation 은 해당 변수에 대한 sample 의 값을 random 하게 섞는 것을 의미한다. 만약 Random Forest 에서 변수 \(x_i\) 의 중요도가 높다면, \(p_i - e_i\) 의 값은 커야 하며 편차가 적어야 한다.</p> \[d_i^m = p_i^m - e_i^m \\ \bar d_i = {1\over m}\sum_{i=1}^m d_i^m \\ s_i^2 = {1 \over m-1} \sum_{i=1}^m (d_i^m - \bar d_i)^2 \\ v_i = {\bar d_i \over s_i}\] <p>\(d_i^m\) 은 m 번째 tree 에서 설명변수 i 에 대해 permutation 전후 OOB error 의 차이를 나타내고, 이에 따른 평균을 분자로, 분산을 분모로 두어 변수의 중요도 \(v_i\) 를 구할 수 있다.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Jonghyun (Thomas) Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>