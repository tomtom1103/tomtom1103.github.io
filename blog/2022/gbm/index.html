<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Ensemble Learning - Gradient Boosting Machine | Jonghyun (Thomas) Lee </title> <meta name="author" content="Jonghyun (Thomas) Lee"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="deep generative models"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/apple-touch-icon-precomposed.png?f3b0e00b51d3560daeef2cfef2a1c566"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tomtom1103.github.io/blog/2022/gbm/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?ac1a8a24b4b1b97e0b04e951186c207f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jonghyun</span> (Thomas) Lee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/Tom_Resume_0225.pdf" target="_blank" rel="noopener noreferrer">cv <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Ensemble Learning - Gradient Boosting Machine</h1> <p class="post-meta"> July 07, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="contents">Contents</h2> <ul> <li>Gradient Boosting Machine: GBM</li> <li>The Gradient</li> <li>Regularization of GBM <ul> <li>Subsampling</li> <li>Shrinkage</li> <li>Early Stopping</li> </ul> </li> <li>Variable Importance in Tree-based GBM</li> </ul> <blockquote> <p>해당 포스트는 2021년 1학기 고려대학교 강필성 교수님의 ‘<a href="https://github.com/pilsung-kang/multivariate-data-analysis" rel="external nofollow noopener" target="_blank">다변량분석</a>’ 강의를 참고하여 작성되었습니다.</p> <p>이미지는 강필성 교수님의 강의 슬라이드에서 발췌하였습니다.</p> <p><del>A+ 감사했습니다 교수님</del></p> </blockquote> <h2 id="gradient-boosting-machine-gbm">Gradient Boosting Machine: GBM</h2> <p>앙상블 기법은 크게 병렬처리가 가능한 bagging, 그리고 불가능한 boosting 방법론들로 나뉜다. Gradient Boosting Machine (GBM) 은 이름에서 알 수 있듯이 Boosting 방법론 중 하나이며, weak model 을 guide 로 학습을 진행한다. Boosting 방법론들 중 shortcomings 라는 단어가 쓰이는데, 이는 Boosting 모델이 이전 단계에서 효과적으로 학습을 진행하지 못했기에 가중치로 삼을 객체를 의미한다. 예시로, AdaBoost 의 shortcomings 는 분류를 잘 못한 샘플 \(i\) 에 대한 가중합이다. GBM 은 shortcomings 를 이름에서 알 수 있듯이 gradient, 즉 경사를 사용한다.</p> <blockquote> <p>GBM 은 머신러닝의 대표적인 3가지 task 인 regression, classificaiton, ranking 에 전부 적용 할 수 있다. 하지만 Gradient, 즉 손실함수에 대한 1차도함수를 계산하는 과정이 있기 때문에 base model 의 손실함수를 미분할 때의 computational complexity 에 따라 difficulty 를 매기기도 한다. 일반적으론 Regression &lt; Classification &lt; Ranking 의 Difficulty 를 가진다.</p> </blockquote> <p>GBM 의 기본 개념은 (Regression 모델을 예시로 두었을 때) 잔차에서 비롯된다. 먼저 설명변수와 종속변수를 잘 설명하는 회귀식을 구축 한 다음, 각 샘플에 대한 잔차를 계산한다. Boosting 방법론이기 때문에 그 다음 모델을 학습하는데, 이때 다음 모델의 종속변수로 이전 단계의 모델의 잔차를 사용한다. <strong>즉, 다음 단계의 모델은 이전 단계의 모델의 에러를 예측하는 모델을 구축하는 것이다.</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/machinelearning/gbm/img1-480.webp 480w,/assets/img/posts/machinelearning/gbm/img1-800.webp 800w,/assets/img/posts/machinelearning/gbm/img1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/machinelearning/gbm/img1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="the-gradient">The Gradient</h2> <p>GBM 에서의 Gradient 라는 워딩은 경사하강법과 연결된다. 예시로, 다중선형회귀분석의 손실함수로 OLS 를 사용한다.</p> \[\textbf{min}\ L = {1 \over 2} \sum ^n_{i=1} (y_i - f(\mathbf{x}_i))^2\] <p>이때 \(\mathbf{x_i}\) 에 대한 손실함수의 1차도함수는 다음과 같다.</p> \[{\partial L \over \partial f(\mathbf{x}_i)} = f(\mathbf{x}_i) - y_i\] <p>이를 잔차에 대해 정리를 하면 다음과 같다.</p> \[y_i - f(\mathbf{x}_i) = - {\partial L \over \partial f(\mathbf{x}_i)}\] <p><strong>즉, 잔차를 손실함수의 negative gradient 로 표현 할 수 있다.</strong> 그렇기에 GBM 의 모델들을 학습시킬 때 종속변수를 각 샘플에 대한 이전 단계의 모델의 negative gradient 로 설정하는 것이 잔차로 설정하는 것과 동일한 효과를 낸다. 결국 GBM 은 매 iteration 마다 전 단계의 gradient 만큼만 학습시키는 것이다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/machinelearning/gbm/img2-480.webp 480w,/assets/img/posts/machinelearning/gbm/img2-800.webp 800w,/assets/img/posts/machinelearning/gbm/img2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/machinelearning/gbm/img2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>위 예시는 base learner 을 regression stump tree 로 사용한 GBM 모델이다. Regression stump tree 는 split 을 기준으로 해당 영역의 샘플들의 평균으로 예측을 해주는 모델이다. 첫째 모델은 하나의 split 을 보이며, 이에 따른 잔차들을 기준으로 두번째 모델을 학습시킨다. Iteration 을 거듭할 수록 값들을 잘 예측하는 것을 볼 수 있으며, 최종 모델의 잔차들은 거의 0에 수렴하는 것을 볼 수 있다.</p> <h2 id="regularization-of-gbm">Regularization of GBM</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/machinelearning/gbm/img3-480.webp 480w,/assets/img/posts/machinelearning/gbm/img3-800.webp 800w,/assets/img/posts/machinelearning/gbm/img3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/machinelearning/gbm/img3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>GBM 의 알고리즘은 위와 같다. AdaBoost 와는 달리, 마지막에 학습된 모든 모델을 aggregate 하지 않고 마지막 iteration \(M\) 에서 구한 모델을 최종 모델로 사용한다. GBM 은 사용하는 손실함수에 따라 알고리즘의 차이가 날 수 있다.</p> <p>Regression 을 위한 GBM 에서는 대표적으로 \(L_1\) Loss, \(L_2\) Loss, Huber Loss, Quantile Loss 를 사용하며,</p> <p>Classification 을 위한 GBM 에서는 대표적으로 Bernoulli Loss, AdaBoost Loss 를 사용한다. 이때 주어진 task 가 Binary classification 이면 AdaBoost 와 동일하게 초기 정답라벨을 \(y \in \{-1,1\}\) 로 사용한다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/machinelearning/gbm/img4-480.webp 480w,/assets/img/posts/machinelearning/gbm/img4-800.webp 800w,/assets/img/posts/machinelearning/gbm/img4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/machinelearning/gbm/img4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>GBM 은 이전 단계 모델의 잔차를 이용한 학습을 한다는 특징이 있기 때문에, 과적합의 고질적인 문제가 있다. 실제 모델엔 어쩔 수 없는 noise 인 \(\epsilon\) 이 포함되있길 마련인데, 잔차에 이런 noise 가 포함되어있을 확률이 높기 때문이다. 그렇기에 GBM 에선 여러가지 정규화 방법론이 쓰인다.</p> <h3 id="1-subsampling">1. Subsampling</h3> <p>첫번째 정규화 방법론은 subsampling 이다. AdaBoost 에선 각 샘플이 뽑힐 확률을 높혀주는 가중치를 통한 복원추출로 데이터셋을 만들지만, 기본적인 GBM 모델은 모든 샘플을 그대로 사용한다. 하지만 모든 샘플을 사용하면 과적합의 위험이 있기에, 각 iteration 마다 데이터셋의 subset 만 사용하는 것이 subsampling 이다. <strong>이때 GBM 에서 사용되는 subsampling 의 특징은 AdaBoost, Bagging 과는 다른 비복원추출이라는 점이다.</strong></p> <blockquote> <p>만약 샘플률을 80%라고 가정하면 매 iteration 마다 샘플의 수가 줄어드는 것이라고 생각할 수 있지만, 아니다.</p> <p>10개의 샘플을 가진 기존 데이터셋에서 두번째 데이터셋을 만들 때 8개가 랜덤추출되며, 8개의 샘플에 대한 잔차가 두번째 모델을 학습시키기 위한 정답라벨이 된다. 이후 세번째 데이터셋은 기존 데이터셋에서의 80% 인 8개를 추출하며, 이 8개의 샘플에 대해 첫번째 모델과 두번째 모델을 합친 모델에서의 잔차를 계산하여 세번째 모델의 학습에 사용된다.</p> </blockquote> <h3 id="2-shrinkage">2. Shrinkage</h3> <p>이전에 살펴본 Shrinkage methods 와 동일한 맥락으로, 특정 객체의 impact 를 줄여주는 가중치를 추가하는 방법론이다. 기본 GBM 에선 매 iteration 마다 만들어지는 모델의 가중치가 동일하다면, Shrinkage method 를 사용한 GBM 은 factor \(\lambda\) 를 추가하여 나중에 만들어지는 모델의 영향력을 감소시킨다.</p> <h3 id="3-early-stopping">3. Early Stopping</h3> <p>다른 인공신경망 모델들에서 사용되는 동일한 방법론으로, Validation Error 을 통해 급격히 증가할것같은 구간에서 알고리즘을 미리 종료하는 방법론이다.</p> <h2 id="variable-importance-in-tree-based-gbm">Variable Importance in Tree-based GBM</h2> <p>Kaggle 에서 진행되는 대회들을 보면, 상위권엔 항상 앙상블 방법론들이 있다. 이중 Random Forest 와 GBM 이 압도적으로 쌍벽을 이루는 경우가 많은데, 이 두 알고리즘은 모델의 성능을 향상시켜줄 뿐만 아니라 변수의 중요도 또한 산출하기 때문이다. RF 와 동일하게 base learner 를 Decision tree (혹은 stump tree) 를 사용 할 시, Influence 를 계산하여 최종 모델을 구축하는데 있어 각 변수의 중요도를 계산할 수 있다.</p> \[Influence_j(T) = \sum^{L-1}_{i=1}(IG_i \times \mathbf{1}(S_i=j))\] <p>\(Influence_j(T)\) 는 단일 tree \(T\) 에서 사용된 변수 \(j\) 에 대한 중요도의 계산식이다.</p> <p>해당 tree \(T\) 에 \(L\) 개의 leaf node 가 있으면, split 의 개수는 \(L-1\) 이다.</p> <p>\(IG_i\) 는 Information gain, 즉 혼잡도의 감소를 나타내며 \(\mathbf{1}(S_i=j)\) 는 단순히 해당 split \(i\) 에서 변수 \(j\) 를 사용했다면 1을 반환해주는 함수다.</p> <p><strong>즉, 단일 tree 에 대한 변수 \(j\) 의 중요도는 해당 tree 에서 해당 변수가 사용되었을 때 Information Gain 의 합이다.</strong></p> \[Influence_j = {1\over M}\sum^M_{k=1}Influence_j(T_k)\] <p>전체 GBM 모델에 대한 변수 \(j\) 의 중요도는 위 식으로 구할 수 있다. 단순히 모든 Tree 에 대한 \(j\) 의 중요도의 평균이다.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Jonghyun (Thomas) Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>