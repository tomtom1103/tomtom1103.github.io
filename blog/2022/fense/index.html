<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Can Audio Captions Be Evaluated With Image Caption Metrics? - FENSE (Zhou, 2022) | Jonghyun (Thomas) Lee </title> <meta name="author" content="Jonghyun (Thomas) Lee"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="deep generative models"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/apple-touch-icon-precomposed.png?f3b0e00b51d3560daeef2cfef2a1c566"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tomtom1103.github.io/blog/2022/fense/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jonghyun</span> (Thomas) Lee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Can Audio Captions Be Evaluated With Image Caption Metrics? - FENSE (Zhou, 2022)</h1> <p class="post-meta"> May 10, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/category/papers"> <i class="fa-solid fa-tag fa-sm"></i> papers</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="contents">Contents</h2> <ul> <li>Introduction</li> <li>Automated Audio Captioning</li> <li>Traditional NLG Metrics <ul> <li>BLEU, ROGUE</li> <li>METEOR</li> </ul> </li> <li>Can Audio Captions Be Evaluated With Image Caption Metrics?</li> <li>Pre-Trained Model Based Metrics</li> <li>Experimental Setup</li> <li>Conclusion</li> </ul> <h2 id="introduction">Introduction</h2> <p>NLG (Natural Language Generation) 와 이를 차용한 Image Captioning 이 발전함에 따라 해당 모델들을 평가하는 metric 또한 발전해 왔다. NLG 에서 사용되는 대표적인 Metric 들은 BLEU, ROUGE, METEOR 등이 있으며, Image Captioning 에서 사용되는 Metric 들은 CIDEr, SPICE, 그리고 이 둘을 혼합한 SPIDEr 정도가 있다. 분명 해당 metric 들은 각 분야에서 모델들을 평가하는데 있어서 좋은 평가지표들이다. 하지만 문제는 Automated Audio Captioning 분야에서도 해당 metric 들이 사용된다는 점이다. NLG, Image Captioning 은 Text-Vision 도메인이고 AAC 는 Text-Audio 도메인임에도 불구하고 말이다. 이에 해당 논문은 AAC 모델을 평가하는데 있어서 기존에 사용되던 metric 보다 훨씬 높은 accuracy 를 보인 FENSE 라는 metric 을 제시한다. 해당 논문은 2022년 4월 IEEE 에서 publish 하였으며, 실제 conference 는 5월 말에 싱가폴에서 열리는 ICASSP 에서 진행된다 <del>굉장히 따끈따끈한 SOTA 다</del>.</p> <h2 id="automated-audio-captioning">Automated Audio Captioning</h2> <p>Automated Audio Captioning (이하 AAC) 는 불과 2021년에 DCASE (Detection and Classification of Acoustic Scene and Events, 국제 최대 오디오 학술대회) 에 새로 추가된 만큼 오디오 딥러닝의 최신 frontier 이다. AAC는 오디오 시그널을 입력으로 받고, 해당 오디오 시그널을 설명하는 Natural Language 를 출력한다. 간단해 보이는 task 이지만 AAC 는 다음과 같은 factor 들을 고려해야 한다.</p> <ol> <li>Identification of Sound Events - 오디오 속 temporal 관심객체의 존재</li> <li>Acoustic Scenes - 오디오 속 scene</li> <li>Spatiotemporal Relationship - 오디오 속 event 의 시공간적 관계</li> <li>Foreground vs. Background discrimination - 오디오 속 어떤 객체를 Foreground 로 취급하고 noise 로 취급할지</li> </ol> <p>AAC 는 보통 영상 속 오디오를 분석하는데 사용되며, 청각장애인의 정보격차의 해소와 미디어의 메타데이터 자동생성을 위해 연구되고 있다. 하지만 여느 딥러닝 분야와 마찬가지로, 확장가능성은 무한하다. AAC 를 모니터링 시스템이나 핸드폰에 탑재하면 실제로 context-aware machine 을 구현할 수 있다.</p> <p>하지만 지금까지 발표된 연구들에서 사용하는 metric 들은 AAC 특화 metric 들이 아니므로 모델들의 성능을 정확히 평가할 수 없다.</p> <h2 id="traditional-nlg-metrics">Traditional NLG Metrics</h2> <p>Natural Language Generation 에서 전통적으로 사용되는 metric 들은 대게 N-gram matching 을 통해 이루어진다.</p> <h4 id="bleu-bilingual-evaluation-understudy-rouge-recall-oriented-understudy-for-gisting-evaluation">BLEU (Bilingual Evaluation Understudy), ROUGE (Recall Oriented Understudy for Gisting Evaluation)</h4> <p>BLEU 와 ROUGE 는 각각 서로 complement 해주는 precision based/recall based metric 이다.</p> <ul> <li>BLEU: Generated Sentence 의 단어 (혹은 n-gram) 가 Referenced Sentence 에 얼만큼 나타나는지를 측정</li> <li>ROUGE: Referenced Sentence 의 단어 (혹은 n-gram) 가 Generated Sentence 에 얼만큼 나타나는지를 측정</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/paper/fense/bleu-480.webp 480w,/assets/img/posts/paper/fense/bleu-800.webp 800w,/assets/img/posts/paper/fense/bleu-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/paper/fense/bleu.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>기존 precision/recall 과 동일하게 BLEU 와 ROUGE 를 조합해서 F1 measure 을 도출할 수 있다.</p> \[{2(BLEU*ROUGE)} \over (BLEU+ROUGE)\] <p>예시를 들어보자.</p> <p>Generated Sentence: “I was generated by an NLG model”</p> <p>Reference Sentence: “I was referenced by a human”</p> <p>두 문장의 공통 unigram 은 “I”, “was”, “by” 가 된다. BLEU 와 ROUGE 를 각각 계산하면,</p> <p>BLEU = 3/7</p> <p>ROUGE = 3/6</p> <p>가 된다.</p> <blockquote> <p><em>BLEU 와 ROUGE 는 프랑스어로 각각 청색, 적색이다. 이름 참 잘지었다.</em></p> </blockquote> <p>하지만 AAC에서 BLEU 와 ROUGE 를 쓰기엔 치명적인 단점이 있다. 계산되는 식을 보면 알 수 있듯이 n-gram 상 아주 미묘한 차이도 다른 n-gram 이기 때문에 error 로 판단한다.</p> <h4 id="meteor-metric-for-evaluation-of-translation-with-explicit-ordering">Meteor (Metric for Evaluation of Translation with Explicit ORdering)</h4> <p>METEOR 은 번역 NLG 에 주로 사용되는 metric 으로, word stem, synonyms, 그리고 simple paraphrases 를 도입함으로써 BLEU 와 ROUGE 의 단점을 완화시킨다. BLEU 와 ROUGE 처럼 precision/recall 을 기반으로 하지만, F measure 에 weight 를 추가하고 단어 순서에 대한 penalty function 을 추가한다.</p> \[P = {m \over w_t}, \ R = {m \over w_r}, \ F_{mean} = {PR \over \alpha P + (1-\alpha)R} \\ \\ Penalty = {\gamma ({c \over u_m})^\beta}, \ where \ 0 \leq \gamma \leq 1 \\ \\ METEOR = F_{mean}(1-Penalty) \\ \\\] <p>m = Number of unigrams in the generated sentence also found in the reference sentence</p> <p>w_t = Number of unigrams in the generated sentence</p> <p>w_r = Number of unigrams in the reference sentence</p> <p>P = Precision</p> <p>R = Recall</p> <p>c = Number of chunks in the generated sentence</p> <p>u_m = Number of unigrams in the generated sentence</p> <p>METEOR 은 기존 BLEU, ROUGE 보다 좋은 성능을 보이지만, contextualized semantics 를 잘 분류하지 못한다는 단점이 있다. 이 외에도 TF-IDF 기반 CIDEr, SPICE, SPIDEr 와 같은 metric 들이 있지만, AAC 의 최종 목적은 사람이 쓴것같은 caption 을 생성하는 것이다. 저자들은 metric 또한 Natural Language 의 semantics 를 이해할 필요가 있기에 Pre-Trained Model Based Metrics 를 사용하는것을 기준으로 삼는다.</p> <h2 id="can-audio-captions-be-evaluated-with-image-caption-metrics">Can Audio Captions be evaluated with Image Caption Metrics?</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/paper/fense/img1-480.webp 480w,/assets/img/posts/paper/fense/img1-800.webp 800w,/assets/img/posts/paper/fense/img1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/paper/fense/img1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>위 예시를 보자. Reference 문장과 오디오 input 만 주어진 상황에서 3개의 Caption 에 대해 metric 을 계산하였다. Caption A 는 오디오를 잘 설명하는 caption이고, Caption B 는 sound event 를 잘 align 하였지만 misprediction 이 있다. Caption C 는 fluency issue 를 포함하였다. Caption A 가 저자들이 의도한 가장 좋은 Caption 임에도 불구하고, 모든 metric 에 대한 점수는 가장 낮았다. 이는 n-gram overlap 이 많을수록 높은 점수를 부여하는 기존 metric 의 문제를 잘 나타낸다. SPICE 는 심지어 점수가 0이다. 이는 SPICE 가 Image Captioning based metric 이기 때문에 오디오에서 노골적으로 spatial object attribute 를 이야기 하지 않는 이상, 0에 수렴한다.</p> <p>사실 해당 실험에서 가장 눈여겨볼 부분은 fluency issue 다. AAC 의 base model 은 NLG 이기 때문에 미완성/문법적으로 맞지 않는 문장들이 생성되길 마련이다. 이는 사람이 보면 한눈에 알아차릴 수 있지만, 기존 metric 들은 이런 error 을 penalize 하지 않는다. 오히려 n-gram overlap 기반 metric 에선 이런 error 가 많은 문장이 더 좋다고 평가하는 모순마저 생긴다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/paper/fense/img2-480.webp 480w,/assets/img/posts/paper/fense/img2-800.webp 800w,/assets/img/posts/paper/fense/img2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/paper/fense/img2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>위 테이블은 AAC 에서 제일 많이 나타나는 오류들이다.</p> <h2 id="pre-trained-model-based-metrics">Pre-Trained Model Based Metrics</h2> <p>최근들어 NLG 에서 사용하는 metric 에 pretrained model 들을 도입하는 연구가 활발히 진행되고 있다. 단순히 통계 기반 metric 이 아니라, 자연어의 context 와 semantics 를 학습한 모델을 metric 에 탑재하려는 아이디어에서 출범했다. BERTScore 은 BERT 모델의 contextualized word embeddings 를 사용한 metric 이며, BLUERT 는 [CLS] 태그 위 선형층을 추가하여 metric 을 사용한다. 이처럼 많은 pretrained model based metrics 중, 저자들은 Sentence-BERT 모델을 metric 으로 차용했다.</p> <p>Sentence-BERT 는 기존 BERT 모델에 One Shot learning 을 위한 샴 네트워크를 탑재한 모델이다. 이 모델은 자연어의 semantic similarity 를 학습하는 특징이 있는데, 저자들은 이 특징을 Metric 에 차용했다. Reference sentence 와 ACC 가 생성한 Generated sentence 를 n-gram 으로 비교하는 것이 아니라, Sentence-BERT 가 학습한 semantics 를 통해 비교하는 방식을 택한 것이다.</p> <p>이와 별개로 앞서 언급한 Fluency Issue 도 address 한다. 다른 metric 들은 문법적으로 어긋난 문장에 penalty 를 부여하지 않는 반면, 저자들은 Error Detector 모델을 학습/탑재하여 metric 에 penalty 를 부여했다. 저자들은 이 Sentence-BERT 와 Error detecting model 을 합친 metric 을 <strong>FENSE (Fluency ENhanced Sentence-BERT Evaluation)</strong> 라고 명명했다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/paper/fense/img2-480.webp 480w,/assets/img/posts/paper/fense/img2-800.webp 800w,/assets/img/posts/paper/fense/img2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/paper/fense/img2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>위와 같은 fluency issue 들을 잡아내는 error detector 모델을 학습시키기 위해 저자들은 Clotho 와 AudioCaps 데이터셋을 사용하였다. 해당 데이터셋 속 training 데이터를 위 테이블의 오류들이 보이게 augmentation 을 진행하고, Error label 을 추가하였다. 이 데이터로 5개의 에러타입의 multi-class classification 과 전체적인 error 의 classification 을 수행하는 BERT 모델을 학습시킨 뒤, 해당 모델을 error detector 로 사용한 것이다.</p> <p><strong>기존 metric 들은 단순히 reference 와 generation 을 비교하지만, FENSE 는 학습된 모델을 metric 에 탑재한다.</strong></p> <h2 id="experimental-setup">Experimental Setup</h2> <p>FENSE 의 성능을 평가하기 위해 저자들은 오디오를 설명하는 두개의 caption 중 더 나은 caption 에 대한 실험을 진행했다. 이때 유의할 점은, 해당 논문은 언제까지나 metric 에 대한 실험이므로 기존 우리가 익숙한 딥러닝 실험과 inference 와는 다른 형태를 띈다. 저자들의 실험 단계는 다음과 같다.</p> <ol> <li> <p>4개의 데이터페어 타입 생성:</p> <p>HC - Human Human correct: 오디오에 대해 두명의 사람이 annotate 한 caption.</p> <p>HI - Human Human incorrect: 오디오에 대해 두명의 사람이 annotate 한 caption 이지만, 하나는 다른 오디오의 caption.</p> <p>HM - Human Machine: 오디오에 대해 사람과 AAC 모델이 annotate 한 caption.</p> <p>MM - Machine Machine: 오디오에 대해 두개의 AAC 모델이 annotate 한 caption.</p> </li> <li> <p>BLEU, METEOR, ROGUE, CIDEr, SPICE, BERTScore, BLEURT, Sentence-BERT (Error detector model 이 없는 FENSE), 그리고 FENSE 로 평가.</p> </li> </ol> <h2 id="conclusion">Conclusion</h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/paper/fense/img3-480.webp 480w,/assets/img/posts/paper/fense/img3-800.webp 800w,/assets/img/posts/paper/fense/img3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/paper/fense/img3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>본 논문에 정확한 알고리즘을 공개하진 않지만, 두개의 AAC benchmark dataset 인 AudioCaps 와 Clotho 로 실험을 진행한 결과, FENSE 는 모든 metric 보다 reference sentence 의 pairwise comparison 점수가 높은 것을 볼 수 있다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/paper/fense/img2-480.webp 480w,/assets/img/posts/paper/fense/img2-800.webp 800w,/assets/img/posts/paper/fense/img2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/paper/fense/img2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>추가적으로 학습한 Error Detection 모델을 탑재하지 않은 Sentence-BERT 와 FENSE 를 비교해본 결과, FENSE 의 accuracy 가 월등히 좋은것으로 나타났다.</p> <p>저자들은 FENSE 가 앞으로 AAC 모델들을 평가하는데 있어 unbiased 한 metric 으로 유용하게 쓰일거라고 기대한다. 하지만 semantic similarity 는 acoustic relevance 와 정확히 직결되지 않기 때문에, 저자들은 image captioning 와 같은 연구들이 진행되길 바라며 논문을 마친다.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Jonghyun (Thomas) Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>