<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Support Vector Machine | Jonghyun (Thomas) Lee </title> <meta name="author" content="Jonghyun (Thomas) Lee"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="deep generative models"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/apple-touch-icon-precomposed.png?f3b0e00b51d3560daeef2cfef2a1c566"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tomtom1103.github.io/blog/2022/svm1/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?0afe9f0ae161375728f7bcc5eb5b4ab4"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jonghyun</span> (Thomas) Lee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Support Vector Machine</h1> <p class="post-meta"> February 08, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="contents">Contents</h2> <ul> <li>Introduction</li> <li>Margin</li> <li>Objective Function of SVM</li> <li>The Lagrangian Multiplier Method and KKT Condition</li> <li>Final Solution</li> </ul> <h2 id="introduction">Introduction</h2> <blockquote> <p>“Nothing is more practical than a good theory.”</p> <p>-Vladimir Vapnik</p> </blockquote> <p>클로드 섀넌 등 위대한 석학들이 거쳐간 벨 연구소에서 (그 당시의) 머신러닝 분류기법의 4번타자인 Support Vector Machine 이 1995년 블라디미르 바프닉의 연구 끝에 탄생했다. Computational Complexity 에 대한 한계가 명확한 신경망을 제치고 SVM 은 효과적인 모델로 자리잡았다. 고차원에서 좋은 성능을 보이고, 샘플 수 보다 차원 수가 높아도 준수한 성늘을 보이고, 본질적으로 Computational Complexity 가 낮은 SVM 에 대한 연구는 계속되었다. 하지만 SVM 을 완성시킨 바프닉이 말한 것 처럼 2000년대 이후 SVM은 신경망을 뛰어넘지 못했고, 딥러닝이 대세가 된 현재 SVM 의 practical application 은 찾기 힘들다. 그렇다고 SVM 을 고대의 유물취급하기엔 kernel trick 등 효과적인 이론들을 머신러닝에 도입시켰고, ‘근본’ 모델 중 하나이기에 배울 수 있는게 많다. 바프닉의 말처럼 <strong>좋은 이론만큼 실용적인 것은 없기 때문이다</strong>.</p> <p>SVM 은 classification 과 regression 에 쓸 수 있는 모델이지만, 대부분 classification 문제에서 사용된다. SVM 은 3가지의 경우에 따라 나뉘는데,</p> <blockquote> <ul> <li>Linear SVM for linearly seperable cases → Hard Margin SVM</li> <li>Linear SVM for linearly nonseperable cases → Soft Margin SVM</li> <li>Kernel methods for Nonlinear Classification</li> </ul> </blockquote> <p>이번 포스팅은 SVM의 전반적 개요와 Hard Margin SVM 을 다룬다.</p> <h2 id="margin">Margin</h2> <p>기존 딥러닝을 괴롭히는 고질병같은 overfitting 은 training data 의 성능이 과도하게 좋으면 일어난다. 이런 overfiited model 은 미래 데이터에 대한 예측이 떨어지기에, generalization 을 챙겨야한다. Overfitting 과 generalization ability 간의 tradeoff 에 대한 연구는 신경망쪽에서 활발하지만, SVM은 본질적으로 statistical learning theory 에 기반해 있기 때문에 train error 이 줄어들면 test error 도 줄어드는 모델이다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/machinelearning/svm1/1-480.webp 480w,/assets/img/posts/machinelearning/svm1/1-800.webp 800w,/assets/img/posts/machinelearning/svm1/1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/machinelearning/svm1/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>2개의 class 를 완벽히 분류하는 hyperplane 은 \(w^T + b = 0\) 이며, \(w\)는 hyperplane 의 법선벡터, \(b\)는 bias 다. \(w\)는 우리가 흔히 알고 있는 hyperplane 의 기울기, \(b\)는 hyperplane 의 y-intercept 라고 생각하면 간단하다.</p> <p>본질적으로 \(N\)차원의 data point 들을 정확히 분류하는 hyperplane 을 찾는 것을 <strong>SVM 을 학습시킨다</strong> 라고 한다. 그렇기 때문에 \(w, b\)를 찾는 것이 궁극적인 목표다. 하지만 feature space 위의 데이터포인트들을 보면 두개의 클라스를 분류하는 hyperplane 은 여러개 일 수 있다. 잊지 말아야 할 것은 어느 머신러닝 모델과 다름없이 hyperplane 을 찾는것은 주어진 training data 를 통해 학습을 시키는 것이기에, training data 를 완벽히 분류할 수 있는 hyperplane 이 많이 존재하더라도 미래에 들어올 데이터도 잘 분류하는 최적의 hyperplane 을 찾는것이 관건이다. 이를 우리는 margin 이라는 개념을 도입해서 풀 수 있다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/machinelearning/svm1/2-480.webp 480w,/assets/img/posts/machinelearning/svm1/2-800.webp 800w,/assets/img/posts/machinelearning/svm1/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/machinelearning/svm1/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>위 그림을 보면 두개의 클래스를 잘 구분하는 hyperplane 이 있다. 각 클래스에서 Hyperplane 과 가장 가까운 점들이 있고, 해당 점들은 또 다른 점선에 놓여있다. 해당 점선들을 각각 plus plane, minus plane 이라 부르며 이 두 plane 의 거리를 margin 이라 한다. Margin 이 클수록 두 클래스는 더 멀리 떨어져 있다는 것을 의미하며, 해당 training set 에서 margin 이 제일 큰 hyperplane 을 찾으면 미래 데이터도 잘 구분할 수 있다는 것을 의미한다. <strong>SVM의 목적은 이 margin 을 최대화 하는 hyperplane 을 찾는 것이다</strong>. hyperplane 의 \(w\) 가 법선벡터임을 여기서 알 수 있다.</p> <h2 id="objective-function-of-svm">Objective Function of SVM</h2> <p>SVM의 목적은 margin 을 최대화하는 hyperplane 을 찾는 것이다. 이를 수식으로 표현하는 과정은 복잡해 보이지만, 생각보다 간단하다. Plus plane 위의 점을 \(x^+\), minus plane 위의 점을 \(x^-\) 라고 부르겠다. 이때 \(w\)는 hyperplane 과 법선벡터이기 때문에 \(x^+\)과 \(x^-\) 를 평행이동관계로 표현 할 수 있다.</p> \[x^+ = x^- + \lambda w\] <p>위 식은 최적화에서의 Basic Search Scheme 과 동일한 개념이다. \(x^-\)를 \(w\) 방향으로 \(\lambda\) 만큼 이동시키면 \(x^+\) 가 된다는 것을 의미한다.</p> <blockquote> <p>이쯤에서 다시 목적이 뭔지 복습하는것이 좋다.</p> <p>Training data 가 주어졌을 때 두개의 클래스를 구분하는 hyperplane 을 찾으려 한다.</p> <p>SVM 에서는 margin 을 최대화 한다면 최적의 hyperplane 을 찾을 수 있다.</p> </blockquote> <p>Margin 을 최대화하는 hyperplane 을 찾으려 한다. 그렇다면 margin 을 계산 가능한 식으로 유도해야 하는데, 위 평행이동관계식으로 margin 을 \(w\)로 표현 할 수가 있다. 먼저 \(\lambda\)를 \(w\)로만 표현하면 다음과 같다.</p> \[x^+ = x^- + \lambda w\\\\ w^Tx^+ + b = 1\\ w^T(x^-+\lambda w)+b=1\\ w^Tx^-+b+\lambda w^Tw = 1\\ -1 + \lambda w^Tw = 1\\\\ \lambda = \frac 2 {w^Tw}\] <table> <thead> <tr> <th>수식</th> <th>설명</th> </tr> </thead> <tbody> <tr> <td>\(x^+ = x^- + \lambda w\)</td> <td>\(x^+, x^-\)의 평행이동관계</td> </tr> <tr> <td>\(w^Tx^+ + b = 1\)</td> <td>plus plane</td> </tr> <tr> <td>\(w^T(x^-+\lambda w)+b=1\)</td> <td>plus plane 에 평행이동관계의 \(x^+\) 대입</td> </tr> <tr> <td>\(w^Tx^-+b+\lambda w^Tw = 1\)</td> <td>분배법칙</td> </tr> <tr> <td>\(-1 + \lambda w^Tw = 1\)</td> <td>\(w^Tx^-+b = -1\) (minus plane) 치환</td> </tr> <tr> <td>\(\lambda = \frac 2 {w^Tw}\)</td> <td>\(\lambda\) 로 정리</td> </tr> </tbody> </table> <p>그 다음 Margin 자체를 유도해야 한다. Margin 은 \(x^+, x^-\) 사이의 거리이기 때문에 L2 norm 으로 전개를 할 수 있다.</p> \[margin = ||x^+ - x^-||_2\\ =||x^- + \lambda w - x^-||_2\\ =||\lambda w||_2\\ =\lambda \sqrt{w^Tw}\\\] <p>Margin 을 전개한 식 \(\lambda \sqrt{w^Tw}\) 에 평행이동관계식에서 도출한 \(\lambda = \frac 2 {w^Tw}\)을 대입하면 다음과 같다.</p> \[margin = \frac 2 {w^Tw} \sqrt{w^Tw}\\ = \frac 2 {\sqrt {w^Tw}}\\ = \frac 2 {||w||_2}\] <table> <tbody> <tr> <td>SVM의 목적은 $$\frac 2 {</td> <td> </td> <td>w</td> <td> </td> <td>_2}$$ 를 최대화 하는걸로 귀결된다. 이 최대화 문제에 역수를 취해 최소화 문제로 만들면 다음과 같다.</td> </tr> </tbody> </table> \[min \ \frac 1 2 \mid\mid w \mid\mid ^2_2\] <p>기존 문제에 역수를 취해 \(min\) 문제로 만든 뒤 L2 norm 의 제곱근을 없애기 위해 목적식에 제곱을 취해주면 <strong>\(w\) 로 표현한 SVM 의 목적식을 얻을 수 있다.</strong></p> <blockquote> \[min \ \frac 1 2 \mid\mid w \mid\mid ^2_2 \rightarrow Objective \ function\] </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/machinelearning/svm1/2-480.webp 480w,/assets/img/posts/machinelearning/svm1/2-800.webp 800w,/assets/img/posts/machinelearning/svm1/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/machinelearning/svm1/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>목적식을 구했다면 제약식을 구할 차례다. SVM 에 새로운 데이터 포인트가 들어온다고 생각해보자. 그렇다면 해당 데이터포인트는 \(+1, -1\) 둘 중 하나의 클라스로 구분 될 것이고, \(+1\)이라면 plus plane 보다 위에 있을 것이고 \(-1\)이라면 minus plane 보다 아래에 있을 것이다. 데이터포인트의 클라스 (혹은 정답 레이블) 는 \(y_i\) 로 표현을 한다면, 위 제약식을 수식으로 쓰면 다음과 같다.</p> <blockquote> \[y_i(w^Tx_i+b)\geq 1, i=1,2,...n \rightarrow Constraints\] </blockquote> <p>이렇게 SVM 의 목적식과 제약식을 구할 수 있다 (편의상 Original Problem 이라 부르겠다).</p> <h4 id="original-problem">Original Problem</h4> <blockquote> \[\min \ \frac 1 2 \mid\mid w \mid\mid ^2_2 \rightarrow Objective \ function\] \[s.t \ y_i(w^Tx_i+b)\geq 1, i=1,2,...n \rightarrow Constraints\] </blockquote> <p>목적식의 변수 \(w\) 에 제곱이 붙어있고, 제약식은 linear 하다. 이렇게 SVM을 최적화에서 그토록 많이 배운 2차계획법 (Quadratic Programming, QP) 문제로 유도한 것이다. 이런 minimizing QP 문제는 convex optimization 이기 때문에, 전역최적해 (Global Minimum) 이 존재한다. 즉, <strong>목적식을 최소화하는 \(w,b\) 가 단 하나만 존재한다는 뜻이다.</strong></p> <h2 id="lagrangian-multiplier-method-and-kkt-condition">Lagrangian Multiplier Method and KKT Condition</h2> <p>SVM 의 Original Problem 은 QP 문제다. QP 문제는 전역최적해가 하나여서 다른 heuristics 가 필요 없다는 점에서는 좋지만, 제약식이 있는 최적화 문제는 추가 단계를 거처야만 풀 수 있다. 제약식이 연립방정식이면 라그랑주 승수법을 통해 새로운 최적화 문제를 풀어야 하고, 제약식이 연립부등식이면 KKT 조건이란 것을 만족하도록 유도해야 한다. SVM 의 Original Problem 을 유도하다 보면 라그랑주 승수법과 KKT 조건을 둘다 사용한다.</p> <h3 id="lagrangian-multiplier-method">Lagrangian Multiplier Method</h3> <p>라그랑주 승수법이란 라그랑주 승수 (Lagrange Multiplier, 보조함수) 를 통해 제약 조건이 있는 최적화 문제를 풀기 위한 방법이다. SVM 의 경우엔 라그랑주 승수법은 제약식을 없애는 역할로 사용된다.</p> <blockquote> <p>보통 라그랑주 승수는 \(\lambda\) 로 표현하는 경우가 많지만, 대부분의 소스와 예제에선 \(\alpha\) 를 사용한다.</p> </blockquote> <p>기존 Original Problem 을 라그랑주 승수법을 통해 Lagrangian Primal 문제로 변환시키면 다음과 같다.</p> <p><strong>Original Problem</strong></p> <blockquote> \[\min \ \frac 1 2 \mid\mid w \mid\mid^2_2 \rightarrow Objective \ function\] \[s.t \ y_i(w^Tx_i+b)\geq 1, i=1,2,...n \rightarrow Constraints\] </blockquote> <p><strong>Lagrangian Primal</strong></p> <blockquote> \[\max_\alpha\min_{w,b} \mathcal{L}(w,b,a) = \frac 1 2 \mid\mid w \mid\mid ^2_2 - \sum \alpha_i(y_i(w^Tx_i+b)-1) \rightarrow Objective \ function\] \[s.t \ \alpha_i \geq 0 \ , \ i=1,2,...,n \rightarrow Constraints\] </blockquote> <p>Original Problem 의 목적식과 제약식이 Lagrangian Primal 에선 \(\alpha\) 를 통해 하나의 목적식으로 합쳐졌고, 기존 제약식의 범위가 1보다 커야 했기 때문에 \(\alpha\) 가 0보다 커야 한다는 제약식이 생긴다. 이 Lagrangian Primal 문제의 목적식이 \(\max\min\) 이기 때문에, \(\max\) 만 존재하는 쌍대문제 (Dual) 로 바꿔 풀어야 한다. 즉, <strong>Primal formulation 을 minmize 후 Dual formulation 을 maximize 하면 된다.</strong></p> <p>Primal formulation 또한 Original Problem 과 같이 QP 이기 때문에 Convex 하고 Continuous 하다. 0에서 최솟값을 갖기 때문에 \(w, b\) 에 대해서 편미분을 해야한다.</p> \[\frac {\partial \mathcal{L}(w,b,\alpha_i)}{\partial w}=0 \rightarrow w = \sum^n_{i=1}\alpha_iy_ix_i\\ \frac {\partial \mathcal{L}(w,b,\alpha_i)}{\partial b}=0 \rightarrow \sum^n_{i=1}\alpha_iy_i=0\] <p>이렇게 두가지 수식을 구할 수 있다. <strong>되짚어보면 우리의 목적은 \(w, b\) 를 구하는 것이다.</strong> 첫번째 수식에 \(w\) 에 대한 정의가 있는데, \(\alpha\) 를 찾아야 하다. 그리고 두번째 수식에서 \(\alpha\) 와 \(y\) 의 곱의 합이 0이 되어야 한다는 것 또한 알게 되었다. 이 두가지 식을 다시 Primal formulation 에 대입을 하면 \(\alpha\) 값을 최대화 시키는것이 목적인 dual 문제로 변환시킬 수 있다.</p> <h3 id="deriving-lagrangian-dual">Deriving Lagrangian Dual</h3> <p>Primal 을 Dual 로 바꾸는것은 어렵지 않다. 편미분을 통해 얻은 두 수식을 Primal 문제에 대입만 해주면 된다.</p> <blockquote> \[Primal: \min_{w,b} \mathcal{L}(w,b,a) = \frac 1 2 \mid\mid w \mid\mid ^2_2 - \sum \alpha_i(y_i(w^Tx_i+b)-1)\] \[w = \sum^n\_{i=1} \alpha_i y_i x_i\] \[\sum^n_{i=1} \alpha_i y_i=0\] </blockquote> <p>Primal 문제를 각 항별로 정리를 해보자.</p> <table> <thead> <tr> <th>수식</th> <th>설명</th> </tr> </thead> <tbody> <tr> <td>\(\frac 1 2 \mid\mid w \mid\mid ^2_2\)</td> <td>Primal 첫째 항</td> </tr> <tr> <td>\(= \frac 1 2 w^Tw\)</td> <td>L2 norm 벡터화</td> </tr> <tr> <td>\(= \frac 1 2 w^T \sum\alpha_jy_jx_j\)</td> <td>\(w\)에 \(\sum^n_{i=1}\alpha_iy_ix_i\) 대입</td> </tr> <tr> <td>\(= \frac 1 2 \sum\alpha_jy_j(w^Tx_j)\)</td> <td>\(w^T\)와 \(x_j\) 묶기</td> </tr> <tr> <td>\(= \frac 1 2 \sum \alpha_jy_j(\sum\alpha_iy_ix^T_ix_j)\)</td> <td>\(w^T\)에 \(\sum^n_{i=1}\alpha_iy_ix_i\) 대입</td> </tr> <tr> <td>\(= \frac 1 2 \sum\sum\alpha_i \alpha_j y_i y_j x^T_i x_j\)</td> <td>전개</td> </tr> </tbody> </table> <table> <thead> <tr> <th>수식</th> <th>설명</th> </tr> </thead> <tbody> <tr> <td>\(-\sum \alpha_i(y_i(w^Tx_i+b)-1)\)</td> <td>Primal 두번째 항</td> </tr> <tr> <td>\(= - \sum\alpha_iy_i(w^Tx_i+b)+\sum\alpha_i\)</td> <td>전개</td> </tr> <tr> <td>\(= - \sum\alpha_iy_iw^Tx_i-b\sum\alpha_iy_i+\sum\alpha_i\)</td> <td>전개</td> </tr> <tr> <td>\(= -\sum\sum \alpha_i \alpha_j y_i y_j x^T_i x_j + \sum \alpha_i\)</td> <td>\(-b\sum\alpha_iy_i\) 는 0이기 때문에 삭제, \(w = \sum^n_{i=1}\alpha_iy_ix_i\) 대입</td> </tr> </tbody> </table> <p>Primal 의 두 항을 정리한것을 합치면 다음과 같은 최종 Dual formulation 을 완성 시킬 수 있다.</p> <p><strong>Lagrangian Dual</strong></p> <blockquote> \[\max_\alpha \sum^n_{i=1} \alpha_i - \frac 1 2 \sum^n_{i=1} \sum^n_{j=1} \alpha_i \alpha_j y_i y_j x^T_i x_j \rightarrow Objective \ function\] \[s.t \ \sum^n_{i=1}\alpha_i y_i = 0, \ \alpha_i \geq 0, \ i=1,2,...,n \rightarrow Constraints\] </blockquote> <p>위 Dual 문제도 Original Problem, Primal formulation 과 동일한 QP다. 그렇기 때문에 목적식을 만족시키는 \(\alpha\) 는 하나만 존재한다. \(\alpha\) 를 구하면 \(w = \sum^n_{i=1}\alpha_iy_ix_i\\\) 를 통해 \(w\) 를 구할 수 있다. 하지만 위에서 언급한 것 처럼 QP 문제에서 제약식이 연립부등식이면 KKT 조건을 만족시켜야 한다.</p> <h3 id="karush-kuhn-tucker-conditions">Karush-Kuhn-Tucker Conditions</h3> <p>일반적인 KKT 조건이라 하면 다음과 같은 3가지의 조건을 의미한다.</p> <ol> <li>모든 독립변수에 대한 미분값은 0이다 - Stationary 조건</li> <li>모든 라그랑주 승수와 제한조건 부등식의 곱은 0이다 - Primal Feasibility 조건</li> <li>라그랑주 승수는 음수가 아니어야 한다 - Dual Feasibiliity 조건</li> </ol> <p>SVM 의 겨우엔, \(w, b,a\) 가 Lagrangian dual 의 최적해가 되기 위해서 KKT 조건을 풀어쓰면 다음과 같이 수식화 할 수 있다.</p> <ol> <li>Stationary</li> </ol> <blockquote> <p>$\frac {\partial \mathcal{L}(w,b,\alpha_i)}{\partial w}=0 \rightarrow w = \sum^n_{i=1}\alpha_iy_ix_i<br> \frac {\partial \mathcal{L}(w,b,\alpha_i)}{\partial b}=0 \rightarrow \sum^n_{i=1}\alpha_iy_i=0$</p> </blockquote> <ol> <li>Primal Feasibility</li> </ol> <blockquote> \[y_i(w^Tx_i+b)\geq 1, i=1,2,...n\] </blockquote> <ol> <li>Dual Feasibility</li> </ol> <blockquote> \[\ \alpha_i \geq 0, \ i=1,2,...,n\] </blockquote> <ol> <li>Complementary Slackness</li> </ol> <blockquote> \[\alpha_i(y_i(w^Tx_i+b)-1)=0\] </blockquote> <p>그런데 자세히 보면 첫 3가지 조건들은 익숙하다. Stationary 조건은 Lagrangian Primal 의 독립변수들을 편미분할때 사용했다. Primal Feasibility 는 Original Problem 의 제약식이고, Dual Feasibility 는 Lagrangian Primal 의 제약식이다. 이미 첫 3가지 KKT 조건을 염두에 두고 Dual Problem 을 유도한 것이다. 그렇기에 마지막 조건인 Complementary Slackness 를 통해 우리는 최종해를 구할 수 있다.</p> <h2 id="final-solution">Final Solution</h2> <p>KKT의 첫번째 조건을 유도하는 과정에서 \(w = \sum^n\_{i=1}\alpha_iy_ix_i\) 를 통해 최종 해를 구할 수 있다는 것을 안다. \(x_i, y_i\) 는 학습데이터이기 때문에 \(\alpha_i\) 만 찾으면 된다. 이는 Complementary Slackness 를 통해 찾을 수 있다.</p> \[\alpha_i(y_i(w^Tx_i+b)-1)=0\] <p>Complementary Slackness 조건을 보면 \(\alpha_i\) 와 \(y_i(w^Tx_i+b)-1\) 중 <strong>반드시 하나는 0의 값을 가져야 한다는 것을 알 수 있다.</strong> 그렇다면 두가지 경우를 생각 할 수 있다.</p> <ol> <li> \[\alpha_i &gt; 0 \ and \ y_i(w^Tx_i+b)=1\] </li> <li> \[\alpha_i = 0 \ and \ y_i(w^Tx_i+b)-1\neq 0\] </li> </ol> <p>첫번째 경우의 두번째 항목을 자세히 보면 margin 의 수식인 것을 알 수 있다. 이는 training set 의 \(x_i\) 가 plus plane 혹은 minus plane 위에 있다는 것을 의미하며, 해당 \(x_i\) 를 <strong>support vector</strong> 이라 부른다.</p> <p>두번째 경우는 \(x_i\) 가 plus plane 혹은 minus plane 위에 있지 않다는 것을 의미한다. 그렇기 때문에 Hyperplane 을 구축하는데 있어서 영향을 미치지 않는다.</p> <p>첫번째 경우에서 찾은 support vector 을 찾으면 margin 을 찾은 것과 다름없다. 왜냐하면 support vector들로 plus plane, minus plane 을 구할 수 있으며 최종적으로 두 Plane 간의 거리가 maximum margin 이기 때문이다.</p> <h4 id="references">References</h4> <ul> <li><a href="https://www.youtube.com/watch?v=qFg8cDnqYCI&amp;list=PLpIPLT0Pf7IoTxTCi2MEQ94MZnHaxrP0j&amp;index=15&amp;t=467s" rel="external nofollow noopener" target="_blank">고려대학교 DMQA 김성범 교수님-핵심 머신러닝 SVM</a></li> <li><a href="https://ratsgo.github.io/convex%20optimization/2018/01/26/KKT/" rel="external nofollow noopener" target="_blank">ratsgo’s blog for textmining-KKT 조건</a></li> <li><a href="https://wikidocs.net/book/1896" rel="external nofollow noopener" target="_blank">Convex Optimization For All</a></li> <li><a href="http://www.yes24.com/Product/Goods/88440860" rel="external nofollow noopener" target="_blank">Machine Learning by Zhou Zhihua</a></li> </ul> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Jonghyun (Thomas) Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>