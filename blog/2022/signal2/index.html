<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Short Time Fourier Transformation, the Mel Filterbank, and MFCC (Audio Signal Processing Pt.2) | Jonghyun (Thomas) Lee </title> <meta name="author" content="Jonghyun (Thomas) Lee"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="deep generative models"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/apple-touch-icon-precomposed.png?f3b0e00b51d3560daeef2cfef2a1c566"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tomtom1103.github.io/blog/2022/signal2/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?ac1a8a24b4b1b97e0b04e951186c207f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jonghyun</span> (Thomas) Lee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/portfolio/">portfolio </a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/Tom_Resume_0225.pdf" target="_blank" rel="noopener noreferrer">cv <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Short Time Fourier Transformation, the Mel Filterbank, and MFCC (Audio Signal Processing Pt.2)</h1> <p class="post-meta"> January 30, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/category/deep-learning"> <i class="fa-solid fa-tag fa-sm"></i> deep-learning</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="contents">Contents</h2> <ul> <li>Feature Extraction</li> <li>Short Time Fourier Transformation</li> <li>The Mel Scale and Mel Filterbank</li> <li>MFCC (Mel Frequency Cepstral Coefficient)</li> </ul> <h2 id="feature-extraction">Feature Extraction</h2> <p>이전 포스팅에서 아날로그 오디오 시그널이 어떻게 디지털 오디오 데이터로 변하는지에 대한 과정을 알아보았다 (ADC). 디지털 오디오를 확보하였다면, 분석을 해야 하는데 첫 단계가 Feature Extraction 이다. 오디오 데이터는 앞서 설명하였듯이 굉장히 많은 주파수가 섞여있는 고차원 데이터이기 때문에, 그냥 분석하기엔 굉장히 어렵다. 오디오 데이터 뿐만 아니라 전반적인 데이터 분석 과정에선 데이터를 잘 설명하는 feature 을 뽑는게 중요한데, 오디오 데이터를 잘 설명해 주는 feature 인 Mel Frequency Cepstral Coefficient 를 뽑는게 최종 목표다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/deeplearning/signal2/1-480.webp 480w,/assets/img/posts/deeplearning/signal2/1-800.webp 800w,/assets/img/posts/deeplearning/signal2/1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/deeplearning/signal2/1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <h2 id="short-time-fourier-transformation">Short Time Fourier Transformation</h2> <h4 id="windowing">Windowing</h4> <p>오디오 데이터에서 MFCC 를 추출하는 첫 단계는 Windowing(혹은 Framing) 이다. 푸리에 변환은 이전 포스팅에서 설명했듯이 <strong>하나의 시그널을 다양한 주기함수의 합으로 표현해 waveform 을 time domain 에서 frequency domain 으로 표현</strong> 하는 방법이라고 설명했다. 하지만 오디오 데이터는 time continuous 한 신호이기 때문에, time domain 에 대한 정보를 보존해야 한다. 일반적인 푸리에 변환을 취해주면 time domain 이 frequency domain 으로 바뀌며 시간에 대한 정보가 사라진다. 이를 극복하는 방법이 푸리의 변환의 일종인 STFT (Short Time Fourier Transform) 이다. 이 STFT 가 MFCC 를 추출하는 windowing 의 핵심 키워드다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/deeplearning/signal2/2-480.webp 480w,/assets/img/posts/deeplearning/signal2/2-800.webp 800w,/assets/img/posts/deeplearning/signal2/2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/deeplearning/signal2/2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Windowing 이란 오디오 데이터를 아주 짧은 주기로 잘라서 그 하나의 조각 (Frame) 에 푸리에 변환을 취해주는 것이다. 각 프레임마다 푸리에 변환을 취해준 결과를 Spectrum 이라 부르는데, 이 Spectrum 의 x 축은 주파수 (Frequency), y 축은 진폭 (Amplitude) 이 된다. 그다음 이 Spectrum 에 몇가지 연산을 더 취해준다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/deeplearning/signal2/3-480.webp 480w,/assets/img/posts/deeplearning/signal2/3-800.webp 800w,/assets/img/posts/deeplearning/signal2/3-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/deeplearning/signal2/3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <ul> <li>Spectrum 의 Amplitude 를 제곱 → Power Spectrum</li> <li>Power Spectrum 의 Amplitude 를 log → Log Spectrum</li> </ul> <blockquote> <p>이때 Log Spectrum 의 y 축이 우리에게 익숙한 소리의 크기를 표현하는 단위인 decibel 이다.</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/deeplearning/signal2/4-480.webp 480w,/assets/img/posts/deeplearning/signal2/4-800.webp 800w,/assets/img/posts/deeplearning/signal2/4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/deeplearning/signal2/4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Log Spectrum 의 x 축은 주파수, y 축은 decibel 이다. 이 Log Spectrum 을 옆으로 차곡차곡 쌓은 것이 Spectrogram 이란 그래프다. x 축은 다시 time domain 이 되고, y 축은 주파수, 그리고 z 축이라고도 할 수 있는 색의 intensity 가 바로 Log Spectrum 의 decibel (Magnitude) 이 되는 것이다.</p> <p><strong>이렇게 오디오 데이터의 time domain 정보를 보존하면서 frequency domain을 구할 수 있다.</strong></p> <h2 id="the-mel-scale-and-mel-filterbank">The Mel Scale and Mel Filterbank</h2> <p>Spectrogram 은 오디오 데이터의 3가지 domain (time, frequency, magnitude) 를 전부 나타낸다. 하지만 아직 이 Spectrogram 을 데이터로 쓸 수는 없다. Spectrogram 의 y 축은 주파수, 즉 헤르츠 스케일로 매핑되어있는데 <strong>사람이 실질적으로 받아들이는 구분점이나 민감도를 반영하지 못한다.</strong> 사람은 낮은 주파수에 비교적 예민하고 높은 주파수에 비교적 둔감하기 때문이다. 예시로 사람은 1kHz 에서 2kHz 까지의 변화는 눈치채지만 5kHz 에서 8kHz 까지의 변화는 눈치채지 못한다. 그렇기 때문에 Mel Scale 이란 것을 사용한다.</p> <h4 id="the-mel-scale">The Mel Scale</h4> <p>Mel Scale 은 사람이 음(音)을, 즉 주파수를 인지하는 기준을 반영한 로그 스케일이다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/deeplearning/signal2/5-480.webp 480w,/assets/img/posts/deeplearning/signal2/5-800.webp 800w,/assets/img/posts/deeplearning/signal2/5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/deeplearning/signal2/5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>주파수를 위 Mel Scale 함수에 대입하면 사람이 인지하는 기준으로 <strong>주파수가 선형적으로 변한다.</strong> 이 Mel Scale 을 선형적이게 구간을 나누어 구현을 한 것이 바로 Mel Filterbank 이다. Mel Scale 을 선형적으로 나누어 다시 Hz 로 바꿔주는 함수다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/deeplearning/signal2/6-480.webp 480w,/assets/img/posts/deeplearning/signal2/6-800.webp 800w,/assets/img/posts/deeplearning/signal2/6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/deeplearning/signal2/6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Mel Filterbank 을 자세히 보면 1kHz 까지 (비교적) 선형적이고 이후엔 지수적으로 증가하는데, 높은 주파수에 둔감한 사람이 실질적으로 받아들이는 변화의 감도를 표현한 것이다.</p> <p><strong>Frequency → Mel Filterbank = Values on the Mel Scale</strong></p> <p><strong>Mel Scale Value → Mel Filterbank = 사람의 민감도를 반영한 Frequency</strong></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/deeplearning/signal2/7-480.webp 480w,/assets/img/posts/deeplearning/signal2/7-800.webp 800w,/assets/img/posts/deeplearning/signal2/7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/deeplearning/signal2/7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>기존에 STFT 로 얻은 Spectrogram 을 이 Mel Filterbank 에 통과시키면 보다 <strong>인간 친화적인</strong> Mel Spectrogram 을 얻을 수 있다.</p> <h2 id="mfcc-mel-frequency-cepstral-coefficient">MFCC (Mel Frequency Cepstral Coefficient)</h2> <p>Mel Spectrogram 구축 후 한 단계만 더 거치면 원래의 목표인 MFCC 를 추출 할 수 있다. 푸리에 변환이 뭔지 복기해보면 하나의 복잡한 함수의 합 (급수) 로 표현하는 방법이다. 이와 다른 방법 중 하나는 Discrete Cosine Transformation (DCT) 라고 한다. Mel Spectrogram 의 각 프레임 또한 함수이기 때문에, 프레임에 DCT 를 취하면 급수로 나타낼 수 있다. 이때 이 급수의 첫 12개의 코사인 계수를 <strong>오디오 데이터의 Cepstral Coefficient</strong> 라고 한다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/deeplearning/signal2/8-480.webp 480w,/assets/img/posts/deeplearning/signal2/8-800.webp 800w,/assets/img/posts/deeplearning/signal2/8-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/deeplearning/signal2/8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>이 12개의 Cepstral Coefficient 과 해당 프레임의 에너지, 즉 평균 데시벨 값까지 총 13개의 feature 을 우리는 오디오 데이터의 주 feature 인 MFCC 라고 부른다. 보통 오디오 데이터 분석을 할 때 어떤 라이브러리를 사용하는지에 따라 상이하지만 13개의 MFCC 를 사용하는 라이브러리도 있고, 여기서 추가적으로 각 프레임의 13개의 feature 에 대해 차분을 추가하고, 2차 차분까지 feature 로 쓰는 라이브러리도 존재한다. 파이썬의 대표적인 오디오분석 라이브러리인 <a href="https://librosa.org/doc/latest/index.html" rel="external nofollow noopener" target="_blank">Librosa</a> 는 2차 차분까지 총 39 개의 feature 를 기본 인자로 사용한다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/deeplearning/signal2/9-480.webp 480w,/assets/img/posts/deeplearning/signal2/9-800.webp 800w,/assets/img/posts/deeplearning/signal2/9-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/deeplearning/signal2/9.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>위 그림은 Mel Spectrogram 에 DCT 를 취해서 MFCC를 Spectrogram 으로 표현 한 그래프다. 여전히 x 축은 time domain, z 축(색깔) 은 Magnitude 를 나타내지만 y 축은 각 MFCC index다.</p> <p>Audio Signal Processing 의 전반적인 흐름은 다음과 같다.</p> <p><strong>Analog Audio Signal → ADC → Digital Audio Signal → Windowing through STFT → Spectrum → Power Spectrum → Log spectrum → Spectrogram → Mel filterbank → Mel Spectrogram → DCT → MFCC</strong></p> <h3 id="references">References</h3> <ul> <li><a href="https://haythamfayek.com/2016/04/21/speech-processing-for-machine-learning.html" rel="external nofollow noopener" target="_blank">Speech Processing for Machine Learning: Filter banks, Mel-Frequency Cepstral Coefficients (MFCCs) and What’s In-Between</a></li> <li><a href="https://www.youtube.com/watch?v=4_SH2nfbQZ8&amp;t=2967s" rel="external nofollow noopener" target="_blank">Mel-Frequency Cepstral Coefficients</a></li> <li><a href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/" rel="external nofollow noopener" target="_blank">Mel Frequency Cepstral Coefficient (MFCC) tutorial</a></li> <li><a href="https://angeloyeo.github.io/2019/07/07/CTFT.html" rel="external nofollow noopener" target="_blank">푸리에 변환(Fourier Transform)</a></li> </ul> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Jonghyun (Thomas) Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>