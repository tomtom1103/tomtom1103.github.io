<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Multiple Linear Regression | Jonghyun (Thomas) Lee </title> <meta name="author" content="Jonghyun (Thomas) Lee"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="deep generative models"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/apple-touch-icon-precomposed.png?f3b0e00b51d3560daeef2cfef2a1c566"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://tomtom1103.github.io/blog/2022/mlr/"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?ac1a8a24b4b1b97e0b04e951186c207f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Jonghyun</span> (Thomas) Lee </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/portfolio/">portfolio </a> </li> <li class="nav-item"> <a class="nav-link" href="/assets/pdf/Tom_Resume_0228.pdf" target="_blank" rel="noopener noreferrer">cv <span class="sr-only">(current)</span> </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-solid fa-moon"></i> <i class="fa-solid fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Multiple Linear Regression</h1> <p class="post-meta"> June 28, 2022 </p> <p class="post-tags"> <a href="/blog/2022"> <i class="fa-solid fa-calendar fa-sm"></i> 2022 </a>   ·   <a href="/blog/category/machine-learning"> <i class="fa-solid fa-tag fa-sm"></i> machine-learning</a>   </p> </header> <article class="post-content"> <div id="markdown-content"> <h2 id="contents">Contents</h2> <ul> <li>Multiple Linear Regression</li> <li>Ordinary Least Square</li> <li>Sum-of-Squares Decomposition</li> <li>Evaluating Regression Models</li> </ul> <blockquote> <p>해당 포스트는 2021년 1학기 고려대학교 강필성 교수님의 ‘<a href="https://github.com/pilsung-kang/multivariate-data-analysis" rel="external nofollow noopener" target="_blank">다변량분석</a>’ 강의를 참고하여 작성되었습니다.</p> <p>이미지는 강필성 교수님의 강의 슬라이드에서 발췌하였습니다.</p> <p><del>A+ 감사했습니다 교수님</del></p> </blockquote> <h2 id="multiple-linear-regression">Multiple Linear Regression</h2> <p><br> 다중선형회귀분석은 역사가 오래된 통계적 기법으로, 정량적 종속변수와 설명변수들의 집합간의 선형 관계식을 찾는 기법이다.</p> \[y = \beta_0 + \beta_1x_1 + \beta_2x_2+ ... +\beta_dx_d + \epsilon \\ \hat y = \hat \beta_1x_1 + \hat \beta_2x_2 + ... + \hat \beta_dx_d\] <p>위 첫번째 식이 데이터셋을 실제로 설명하는 회귀식이고, 두번째 식이 다중선형회귀분석을 통해 추정된 회귀식이다. 종속변수는 설명변수들의 1차항의 결합으로 표현할 수 있고, 첫번째 식의 epsilon 은 실제 데이터에서 볼 수 있는 변동성, 혹은 noise 라고 한다.</p> <p>다중선형회귀분석을 진행하는 이유는 크게 두가지로 나눌 수 있다.</p> <table> <thead> <tr> <th style="text-align: left"> </th> <th style="text-align: center">Explanatory Regression</th> <th style="text-align: center">Predictive Regression</th> </tr> </thead> <tbody> <tr> <td style="text-align: left">목적</td> <td style="text-align: center">종속변수와 설명변수간의 관계를 설명</td> <td style="text-align: center">미래 데이터를 예측하기 위한 회귀식 추정</td> </tr> <tr> <td style="text-align: left">목표</td> <td style="text-align: center">각 설명변수의 회귀계수 \(\beta\) 추정</td> <td style="text-align: center">미래 데이터 예측</td> </tr> <tr> <td style="text-align: left">방법론</td> <td style="text-align: center">Goodness of fit, \(R^2\), Residual Analysis, p-values</td> <td style="text-align: center">학습 데이터셋으로 모델학습</td> </tr> <tr> <td style="text-align: left">분석대상</td> <td style="text-align: center">\(\hat \beta\)</td> <td style="text-align: center">\(Y\)</td> </tr> </tbody> </table> <p>즉 Explanatory Regression 은 통계적 기법이고, Predictive Regression 은 머신러닝적 기법이다.</p> <h2 id="ordinary-least-square">Ordinary Least Square</h2> <p><br> OLS (Ordinary Least Square) 기법, 혹은 최소자승법은 Explanatory Regression 에서 \(\hat \beta\) 를 구하는데 사용하는 기법 중 하나이며, 가장 대중적인 방법론이다.</p> \[Actual = y = \beta_0 + \beta_1x_1 + \beta_2x_2+ ... +\beta_dx_d + \epsilon \\ Predicted = \hat y = \hat \beta_1x_1 + \hat \beta_2x_2 + ... + \hat \beta_dx_d \\ min\ {1 \over 2} \sum(y_i - \hat y_i)^2 = {1\over2}(y_i - \hat \beta_0 + \hat \beta_1x_{i1} + \hat \beta_2x_{i2}...+\hat \beta_dx_{id})^2\] <p>OLS 는 실제값과 추정치의 차이의 제곱합을 통해 구한다. 부호를 상쇄시키기 위해 절댓값의 합과 제곱합 두개의 선택지가 있지만, 절댓값은 미분이 불가능하다는 점에서 OLS 가 가장 많이 쓰인다. 위 식이 어떻게 계산되며 어떤 결과물이 확 와닿지 않지만, 행렬연산을 통해 시각화하면 이해가 간다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/machinelearning/mlr/img1-480.webp 480w,/assets/img/posts/machinelearning/mlr/img1-800.webp 800w,/assets/img/posts/machinelearning/mlr/img1-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/machinelearning/mlr/img1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>X 는 설명변수이고, y 는 종속변수다. 이때 X 의 크기는 n x d+1 이며, +1 을 해주는 이유는 선형회귀식의 상수항 \(\hat\beta_0\) 을 표현하기 위함이다. 그렇기 때문에 X 의 첫번째 열은 전부 1로 채워진다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/machinelearning/mlr/img2-480.webp 480w,/assets/img/posts/machinelearning/mlr/img2-800.webp 800w,/assets/img/posts/machinelearning/mlr/img2-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/machinelearning/mlr/img2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>\(\hat y\) 은 \(X\hat\beta\) 이기 때문에 위와 같이 쓸 수 있다. Minimize을 하기 위해 OLS 의 결과는 scalar 값이 나와야 하기 때문에, 앞의 행렬의 전치행렬을 곱해준다. 이후 Min 값을 구하기 위해 \(\hat \beta\) 에 대해 양변을 편미분해주면 그 값은 0이 나와야하기 때문에 \(\hat \beta\) 에 대해 정리를 해주면 다음과 같은 행렬연산식을 얻을 수 있다.</p> \[\hat\beta = (X^TX)^{-1}X^Ty\] <p>위 행렬연산을 통해 \(\hat \beta\) 라는 d+1 x 1 행렬을 구할 수 있으며, 해당 행렬이 데이터셋의 회귀계수다. <strong>이는 dataset 을 가장 잘 설명하는 회귀식이 유일하게 하나만 명시적으로 존재한다는것을 의미한다.</strong> 즉, OLS 는 우리가 익숙한 머신러닝적 기법을 통한 weight update 방식이 아닌, explicit solution 이 존재하는 기법이다.</p> <p>하지만 OLS 를 통해 \(\hat \beta\) 를 구할 때, 다음과 같은 조건을 만족해야만 구한 \(\hat \beta\) 이 best estimate 이란 것을 보장할 수 있다.</p> <ol> <li>\(\epsilon\) follows the normal distribution.</li> <li>The linear relationship is correct.</li> <li>The cases are independent from each other.</li> <li>Homoskedasticity - The variability in Y values for a given set of predictors is the same regardless of the values of the predictors.</li> </ol> <p>1번, 4번 조건을 잔차도를 통해 판단하는 기법은 다음과 같다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/machinelearning/mlr/img4-480.webp 480w,/assets/img/posts/machinelearning/mlr/img4-800.webp 800w,/assets/img/posts/machinelearning/mlr/img4-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/machinelearning/mlr/img4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>위의 예시를 보자. 실제 데이터는 \(y=2x+\epsilon\) 의 선형관계가 있고, 회귀분석을 통해 \(y=2.1456x + 4.9137\) 란 선형회귀식을 구했다. 이때 선형회귀식과 실제 데이터 사이의 차이를 잔차라고 하는데, 이 잔차 (\(\epsilon\)) 는 정규분포를 따라야 한다. 이를 확인하기 위해 일반적으로 QQ Plot of Residuals 라는 것을 그린다. 해당 plot 의 x 축은 정규분포의 \(\sigma\) 를 나타낸다. 그렇기 때문에 QQ Plot 의 중앙선에서 잔차들이 신뢰구간 95% 이전부터 벗어난다면, 잔차들은 정규성을 띄지 않는다고 판단할 수 있다. Best practice 론 x 축에서 \(\pm2\) 구간 내에 잔차들이 이탈하지 않는다면 정규성을 띈다고 가정한다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/machinelearning/mlr/img5-480.webp 480w,/assets/img/posts/machinelearning/mlr/img5-800.webp 800w,/assets/img/posts/machinelearning/mlr/img5-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/machinelearning/mlr/img5.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Homoskedasticity 를 확인하는 법은 위의 예시를 통해 이해할 수 있다. x 축에 회귀식으로 예측한 \(\hat y\), y 축엔 잔차를 도식해보면 \(\hat y\) 값의 크기와 상관없이 잔차가 일정한 예시는 d 밖에 없다. 즉, 위 예시 중 Homoskedasticity 조건을 만족하는 회귀식은 d 다.</p> <h2 id="sum-of-squares-decomposition">Sum-of-Squares Decomposition</h2> <p><br> OLS 를 통해 회귀식을 추정한 뒤, 해당 회귀식의 성능이 얼만큼 좋은지에 대한 정량적인 지표를 ‘Goodness of Fit’ 이라 한다. 이 Goodness of Fit 의 지표 중 하나인 \(R^2\) 을 구하기 위해 먼저 Sum of squares decomposition 을 이해해야 한다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/machinelearning/mlr/img6-480.webp 480w,/assets/img/posts/machinelearning/mlr/img6-800.webp 800w,/assets/img/posts/machinelearning/mlr/img6-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/machinelearning/mlr/img6.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <table> <thead> <tr> <th style="text-align: center">SST (Sum of Squares Total)</th> <th style="text-align: center">SSR (Sum of Squares Regression)</th> <th style="text-align: center">SSE (Sum of Squares Error)</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">실제값과 평균의 차의 제곱합</td> <td style="text-align: center">추정값과 평균의 차의 제곱합</td> <td style="text-align: center">실제값과 추정값의 제곱합</td> </tr> </tbody> </table> <p>OLS 로 \(\hat \beta\) 를 구할 때 실제값과 추정값의 제곱합을 최소화하는 조건으로 \(\hat \beta\) 가 구해진다. 반면 SST 는 실제 데이터 값과 실제 데이터 값의 평균의 차로 계산되기 때문에 구해진 \(\hat \beta\) 값이 어떻든 변하지 않는다. 그렇기에 OLS 를 통해 구한 \(\hat \beta\) 값의 성능이 좋다면, SSE 는 작을 것이고 SSR 의 값은 클것이다. 이 원리로 해당 모델의 성능을 평가하는 것이다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/machinelearning/mlr/img7-480.webp 480w,/assets/img/posts/machinelearning/mlr/img7-800.webp 800w,/assets/img/posts/machinelearning/mlr/img7-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/machinelearning/mlr/img7.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Sum of Squares decomposition 으로 구한 회귀식의 평가지표중 하나인 \(R^2\) 을 구할 수 있다.</p> <ul> <li> \[0 \leq R^2 \leq 1\] </li> <li>\(R^2=1\) 일때 구한 회귀식이 모든 데이터포인트를 지난다</li> <li>\(R^2=0\) 일때 설명변수와 종속변수간 <strong>그 어떤 선형적인 관계가 없다.</strong> </li> </ul> <blockquote> <p>\(R^2\) 값을 구했을 때 값이 높다는건 분석가가 일을 잘했다는 것을 의미하는게 아니다. 앞서 언급하였듯이 하나의 데이터셋으로 구한 \(\hat \beta\) 는 closed form solution 이기에 항상 같은 결과가 나오기 때문에 \(R^2\) 값도 동일하다. \(R^2\) 값이 높게 나왔다면 \(X\) 와 \(y\) 간 선형적 관계가 강하다는것을 의미할 뿐이다.</p> </blockquote> <p>\(R^2\) 값은 구한 선형회귀식의 좋은 평가지표이지만, 치명적인 단점이 존재한다. 만약 데이터셋의 설명변수의 개수를 늘린다면 설명변수의 실제 설명력과 관계없이 \(R^2\) 값은 단조증가하기 때문이다. 그렇기에 분석이 잘 된것이라 오해하기 쉽다. 이 문제를 해결하기 위해 Adjusted \(R^2\) 을 사용한다.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/posts/machinelearning/mlr/img8-480.webp 480w,/assets/img/posts/machinelearning/mlr/img8-800.webp 800w,/assets/img/posts/machinelearning/mlr/img8-1400.webp 1400w," sizes="95vw" type="image/webp"></source> <img src="/assets/img/posts/machinelearning/mlr/img8.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Adjusted \(R^2\) 값을 구하는 식엔 필요 없는 설명변수가 추가될때를 대비한 일종의 장치가 마련된것을 볼 수 있다. \(p\) 는 변수의 수를 의미하며, 종속변수를 설명하는데 있어 insignificant 한 설명변수가 추가된다면 \(p\) 가 +1 이 되며 \(R^2\) 값은 증가하지 않는다. <strong>반대로 말해서 변수를 추가해서 얻는 이득이 확실할때만 \(R^2\) 값이 증가한다.</strong></p> <h2 id="evaluating-regression-models">Evaluating Regression Models</h2> <p><br> OLS 를 통해 선형회귀식을 구하고, QQ plot of Residuals 를 통해 잔차의 정규성을 검증하고, Homoskedasticity 를 검증한 뒤 \(R^2\) 와 각 변수에 대한 p-value 를 구한다. 이후 Evaluating Regression Models 의 AE, MAE, MAPE, MSE, RMSE 를 접하는데, 처음 다중선형회귀분석을 접했을 때 가장 헷갈렸던 부분이다. 선형회귀식의 성능을 검증하는 부분인데, 앞서 \(R^2\) 값과 p-value 가 이미 성능을 검증하는데 사용되는 지표라고 배웠기 때문이다.</p> <p>결론부터 말하자면 Sum of Squares Decomposition 으로 구한 \(R^2\), p-value, 그리고 잔차도 검증은 Explanatory Regression 에서 사용되는 고전적인 통계학적 성능평가지표이고, MAE, MSE 등등은 Predictive Regression 에서 사용되는 머신러닝적 성능평가지표이다. 어떠한 관점에서 다중선형회귀를 진행하는지에 따라 용도가 달라지며, MAE, MSE 는 다른 머신러닝 모델 (ex. Random Forest, AdaBoost, SVM Regressor 등) 과 회귀모델의 성능을 비교하는데 사용된다. <strong>즉, \(R^2\), p-value 는 다중선형회귀분석의 범주 내에서의 성능을 평가하는데 사용되며, MAE, MSE 는 다른 회귀모델과 성능을 비교하는데 사용된다.</strong></p> <blockquote> <p>그렇기 때문에 단락의 이름이 Evaluating Multivariate Linear Regression Models 가 아닌 Evaluating Regression Models 이다. 다른 Regression Models 에도 사용할 수 있는 다용도의 평가지표를 소개하기 때문이다.</p> </blockquote> <p><strong>Average Error</strong></p> \[{1 \over n}\sum (y_i - \hat y_i)\] <p>첫번째 평가지표는 Average Error 이다. 실제 값과 예측값의 차이의 합의 평균이다. 부호의 효과 때문에 회귀모델의 성능이 좋다는 착각에 빠지기 쉽기 때문에 사용되지 않는다.</p> <p><strong>Mean Absolute Error (MAE)</strong></p> \[{1 \over n} \sum |y_i - \hat y_i|\] <p>MAE 는 Average Error 에 절댓값을 씌운 평가지표다. Average Error 보다는 괜찮은 지표이지만 두개의 회귀모델을 비교할 때 relative difference 를 보여주긴 어렵다.</p> <p><strong>Mean Absolute Percentage Error (MAPE)</strong></p> \[{1 \over n} \sum |{y_i - \hat y_i} / y_i|\] <p>MAPE 는 MAE 의 단점을 보완해준다. 실제값과 예측값의 차이를 실제값으로 나눠줌으로써 실제 값에서의 오차를 나타내준다. 그렇기 때문에 QC 같은 상대적 오차가 절대적 오차보다 중요한 분야에서 많이 쓰인다.</p> <p><strong>Mean Sqared Error (MSE), Root Mean Squared Error (RMSE)</strong></p> \[MSE = {1\over n} \sum (y_i - \hat y_i)^2 \ , \ RMSE = ({1\over n} \sum (y_i - \hat y_i)^2)^{1/2}\] <p>MSE, RMSE 는 부호를 상쇄시키기 위해 절댓값을 사용한 앞선 지표들과 달리 제곱합을 사용한다. MAE, MAPE 는 실무적인 관점에서 직관적인 이해가 용이하지만 모든 점에서 미분 불가하기 때문에 Analytically untractable 하다. 그렇기에 연구목적에서는 MSE, RMSE 를 더 많이 사용한다. 참고할 점은 사용한 데이터셋이 정규분포를 따르고 극단적인 outlier 가 없다면 RMSE 와 MAE 값은 근사한다.</p> <p>MAE, MAPE, MSE, RMSE 모두 다른 회귀모델과의 비교를 위해 좋은 평가지표이지만, 상황에 따라 어떤 성능평가지표를 사용하는지는 분석가의 역량이다.</p> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Jonghyun (Thomas) Lee. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2930004b8d7fcd0a8e00fdcfc8fc9f24"></script> <script defer src="/assets/js/common.js?da39b660470d1ba6e6b8bf5f37070b6e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>